%%
%% Copyright 2007, 2008, 2009 Elsevier Ltd
%%
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%%

%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%% SP 2008/03/01
%%
%%
%%
%% $Id: elsarticle-template-num.tex 4 2009-10-24 08:22:58Z rishi $
%%
%%
\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% if you use PostScript figures in your article
%% use the graphics package for simple commands
%% \usepackage{graphics}
%% or use the graphicx package for more complicated commands
%% \usepackage{graphicx}
%% or use the epsfig package if you prefer to use the old commands
%% \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}

%Package for adding notes/comments/and tracking changes
\usepackage[inline]{trackchanges}
\addeditor{JTF}
\addeditor{Michael}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
%% \usepackage{lineno}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%
%% \biboptions{comma,round}

% \biboptions{}


\journal{Computer Methods in Applied Mechanics and Engineering}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
\author{Michael D. Brothers}
\author{John T. Foster\corref{cor1}}
\author{Harry R. Millwater}
\address{Mechanical Engineering Department, The University of Texas at San Antonio}

%% \ead{email address}
%% \ead[url]{home page}
\cortext[cor1]{Corresponding Author: john.foster@utsa.edu}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\title{A comparison of different methods for calculating tangent-stiffness matrices in a massively parallel computational peridynamics code.}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \address[label1]{<address>}
%% \address[label2]{<address>}

\begin{abstract} %% Text of abstract Shown is a retrospective comparative study of tangent-stiffness
matrix calculation methods including a newly developed Complex Taylor Series Expansion
(CTSE)-based ``complex-step'' method alongside established methods: forward-difference,
central-difference and automatic differentiation.  Tangent-stiffness matrix is a term from
computational mechanics which here refers to the Jacobian matrix as used in first-order
Newton-Raphson methods for the solution of non-linear algebraic systems of equations developed
to describe physical or theoretical systems studied in math, science, engineering and business.
To perform the comparative study, the above tangent-stiffness calculation methods were applied
in a massively parallel computational peridynamics code developed at Sandia National Labs,
called \emph{Peridigm}. In the comparative study, for each datum for each run, complex-step was
multiple orders of magnitude more accurate the finite-difference methods, according to a common
comparison made to the automatic-differentiation method datum. However, for the implementation
contemporary to the study, complex-step was also the slowest method among the four for each
datum for each run. A mathematical definition of \change[Michael]{the methods}{complex-step and
and automatic differentiation}, description of the computer implementations and justification
of the comparative study precede results for clarity. The intended audience of this paper
includes researchers, professionals and students, \change[Michael]{therefore conceptually explicit language
appealing to multiple levels of understanding is used}{therefor readers are directed to
selected background sources in the text for well-known concepts while less familiar concepts
are derived in-paper with an attempt made to lay steps out plainly for those in disparate fields
or who are new to their studies.}  \end{abstract}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword
Newton's method \sep Newton-Raphson \sep numeric differentiation \sep complex-step \sep finite-difference \sep automatic-differentiation \sep Jacobian \sep tangent-stiffness
%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)
\MSC [2010] 65D25 
\end{keyword}

\end{frontmatter}

%%
%% Start line numbering here if you want
%%
% \linenumbers

%% main text
\section{Acknowledgements}
This work was funded in part by a grant from the United States Airforce Office of Scientific
Research. This work is affiliated with the Center for Simulation Visualization and Real-Time
Prediction at the University of Texas at San Antonio.


\section{Introduction}
%\subsection{Motivation}
\label{sec:intro} 

In order to maintain the quadratic convergence properties of the first-order Newton's method in
quasi-static nonlinear analysis of solid structures it is crucial to obtain accurate,
algorithmically consistent tangent-stiffness matrices. For an extremely small class of nonlinear
material models, these consistent tangent-stiffness operators can be derived analytically; however,
most often in practice, they are found through numerical approximation of derivatives. 

A goal of this comparative study, was to develop and evaluate a new, accurate, and practical method
for calculating tangent-stiffness matrices against established methods.  This new method is based on
a complex number Taylor series expansion and refered to as CTSE or the ``complex-step'' method.  The
distinction of `accurate' is defined by comparison of the new method to popular finite differencing
techniques used for computing tangent-stiffness matrices and with the exact algorithmically
consistent derivatives computed with \emph{automatic differentiation}.

It was thought that in addition to comparative information regarding the new method, comparisons of
exclusively finite-difference, central-difference and automatic differentiation could prove valuable
for developers or analysts not necessarily considering complex-step but needing data to help decide
whether or not to invest in automatic-differentiation vs. finite-difference.  A comparison and
discrimination of the methods was to be achieved through application and the measurement of results
in-situ. The results would largely be treated as deterministic consequences of method choice. There
was not an attempt made to mathematically prove the suitability of one method over another of the
methods. The study was retrospective, as explained in section {SECTION} of the instant paper. 

The scope of the application component of the study was limited to the Linear Peridynamic Solid
material model implemented in the computational peridynamics code, \textit{Peridigm}. Identifying a
specific application provided a practical framework for implementing the new method and solving
engineering problems to generate the data needed to compare the methods. In particular,
\emph{Peridigm} was chosen cause it combined several helpful characteristics: The reliance on
Newton's method and tangent-stiffness matrices for solving quasi-static implicit problems, prior
inclusion of finite-difference, central-difference and automatic-differentiation based methods
needed for comparison to a new method that would be developed, and being largely based in the Ansi
standard version of the popular C++ programming language.

The aim of this paper is to introduce new information, but also to compile prior art to serve as a
general reference for solving non-linear systems. \remove[Michael]{The former is mainly intended for
an audience of experienced researchers and professionals who are seeking novel tools, the later
is intended for an audience of undergraduate to graduate level students who would benefit from a
source detailing basic methodology.  According to this aim, concepts are presented in multiple
levels of detail to accommodate both a broad and deep audience.} After giving background on the
\add[Michael]{possibly less familiar} differentiation techniques underlying the methods to be
discussed here, presented in this paper is: a description of tangent-stiffness matrices and how to
use them to solve non-linear systems, detailed directions for producing tangent-stiffness matrices
with each the methods identified above \remove[Michael]{preceded by background as it becomes
necessary}, a description and justification of a new method, called 'complex-step' for calculating
tangent-stiffness matrices, a description of implementing complex-step in the software package used
in the comparative study, a description of the parameters of the comparative study conducted to rank
the methods, a presentation and analysis of the results of the comparative study, and
conclusions and thoughts on potential future work. Finally, the reader is referred to the
corresponding author's website for c++ source-code and data necessary to reproduce the results
presented here. Additionally included on the website is a C+ template library with classes for
solving non-linear systems using the techniques dicussed here, as well as two validation and
verification example problemsi which show how to use the library.

\subsection{Differentiation Techniques}

This subsection contains background information on the differentiation techniques underlying the
tangent-stiffness matrix calculation methods compared in the study. Since first-order finite
difference techniques are considered well known, they are not described where. Instead the
reader is referred to \cite[Chap. 4.1.3]{chapra2010}.

\subsubsection{Basis of New Method, 'Complex Step', in CTSE}  
\note[Michael]{TODO: cite lyness and moler, complete section}

During a literature review in preparation of material distinct from the instant paper, one of the
instant authors identified a work presenting an application of the prior work of applied
mathematicians Lyness and Moler to the numerical approximations of the first derivatives of real
functions. First called Complex Taylor Series Expansion (CTSE) in (Lyness and Moler) and later by
\cite{voorhees2011complex}, where Dr. Millwater is one of the  instant authors, the use of the term and
its acronym are adopted out of deference to their  originators. Lyness and Moler described CTSE
based on  approximating the Taylor Series Expansion of a function F, where $F:R^1 \rightarrow R^1$  about a
complex point and solving for the first derivative. The application of Lyness and Moler's work
demonstrated by Squire and Trap showed that there was empirical evidence that a CTSE based numerical
derivative formula could resist the accuracy reducing phenomenon of subtractive cancellation in
circumstances where central-difference and forward-difference were  demonstrated to succumb to
subtractive-cancellation.       
 
\remove[Michael]{
\subsubsection{First-order Finite-difference}

The types of first-order finite-difference that will be discussed are forward-difference and
central-difference. CTSE and these finite-difference techniques are similar in that they have been
developed by algebraically solving approximated Taylor series expansions for first derivative terms.
}

\subsubsection{Differentiation Technique: Automatic-differentiation} 
\label{ADsubsection}

Automatic-differentiation or 'AD' is a computerized method for computing exact derivatives based the
chain-rule from calculus. AD takes advantage of the fact that any
mathematical function executed on a computer, no matter how complicated, is a "composition of simple
operations" (add, multiply, power, transcendental and the like) each having known analytical derivatives
\cite{ref-sacado-presentation}. For reference, the AD
implementation used in the study is the "Sacado" package from the "Trilinos" library developed out of Sandia
National Labs \cite{ref-Sacado}.

The way an AD system works is by first evaluating the innermost function of the composition, then presenting
that function's output as input to the next level function until all levels are complete, in a way no
different from how a normal computer program or human would evaluate composition functions.
AD departs in that as each nested function is evaluated, the function's
partial derivative with respect to the designated variables of the given input is also calculated.
This is possible because the elementary math functions are hard-coded into the
AD source-code along with their analytical derivatives, and linked by special instructions, so that when
the elementary math functions are called upon for computation, their partial derivatives may be computed and
stored in a sequence. The AD system then multiples the final sequence of partial derivatives together to
produce the exact equivalent to taking a partial derivative of the corresponding composition
function made up of the elementary functions with respect to a designated variable at a particular
value. It is obviously, but bears mentioning, that the AD system could simply store one value for partial
derivatives, modifying it as appropriate for every function evaluation rather than keeping a sequence.
In the literature, the AD scheme described here is called 'forward automatic-differentiation'. For
brevity, only forward AD will be covered since it is pertinent to the study, however the reader is referred
to the introduction section of \cite{ref-AD-methods} and its references list for further information on AD,
particularly \cite{ref-on-AD} which is foundational. 

To make the above description complete, this subsection concludes with an example forward AD process, with 
an emphasis on how general purpose information known to the computer is specialized with additional
information from the problem and used to carry out AD:

\begin{enumerate}
%Set up example problem. The example is meant to walk the reader through the forward AD algorithm
%while encouraging them to rely on their understanding of the chain rule
\item Take an example composition function $f(x) = (sin(cos(x)))^2$\\

\item Suppose we want to know $\frac{df}{dx} \mid_{x_0}$ where $x_0$ is a particular value of $x$. \\

\begin{enumerate}
%This first block of subitems is meant to establish that the values of x that the user cares about
%about evaluating functions for are included within the set of numbers that functions g,h,k are allowed 
%to deal with, and that nested functions can take each other as input, where s may not necessarily equal
%x.
\item Given that: \\
\label{given}
\begin{enumerate}
\item $S \in R^1$ 
\item $X \in S$ 
\item $g, h, k \in H: S \rightarrow S$ 
\item $\forall s \in S : g(s) = {s}^2$, $h(s) = sin(s)$, $k(s) = cos(s)$ 
\item
\label{composition}
$\forall x \in X : f(x) = g(h(k(x)))$ \\
\end{enumerate}

\item Given that the computer is programmed with some mathematical definitions:\\
\begin{enumerate}
\item
\begin{enumerate}
\item
%The computer has at least definitions for these functions and variables
$u, v, w \in H: R^3 \rightarrow R^1$ \\
\item
$s,a,b,x \in R^1$\\
\end{enumerate}

\item
%AD doesn't return symbolic epressions for derivitives that can be re-evaluated.
%Instead, generic versions of functions and evaluations of their partials are defined together.
particular values can be identified: \\
$s = s_0,..,s_n,...s_{\infty} \mid n=[0,\infty)$ \\ 
and similarly for the other variables $a,b,x$ \\
\item
functions $u,v,w$ and their partial derivatives w.r.t $s$ are defined such that: \\
\begin{tabular}{l | l}
\multicolumn{2}{c}{for $a, b, s$ equal to $a_n, b_n, s_n$:} \\ \hline
$u \mid _{a_n, b_n, s_n} = a_n \cdot s_n^{b_n}$ & $\frac{\partial{u}}{\partial{s}} \mid _{a_n, b_n, s_n} = a_n \cdot b_n \cdot s_n^{b_n - 1}$ \\
$v \mid _{a_n, b_n, s_n} = a_n \cdot sin(b_n \cdot s_n)$ & $\frac{\partial{v}}{\partial{s}} \mid _{a_n, b_n, s_n} = a_n \cdot b_n \cdot cos(b_n \cdot s_n)$\\ 
$w \mid _{a_n, b_n, s_n} = a_n \cdot cos(b_n \cdot s_n)$ & $\frac{\partial{w}}{\partial{s}} \mid _{a_n, b_n, s_n} = -a_n \cdot b_n \cdot sin(b_n \cdot s_n)$\\
\end{tabular} 
\end{enumerate}
%a comment
\item 
\label{abcvalues}
%General definitions of functions are specialized by arguments. 
Given that it is possible to describe $f(x) = (sin(cos(x)))^2$ in terms the computer understands by inputting
$f(x)$ such that the computer stores an equivalent statement $f(x) = u(a, b, s) \mid_{arguments}$,
iff the arguments of $u,v,w$ are chosen such that $u,v,w$ approximate $g,h,k$ as follows:\\
\begin{tabular}{l l l l | c}
	Function & a & b & s & Approximates Function\\ \hline 
	$u$ & $1$ & $2$ & $v$ & $g$\\ \hline 
	$v$ & $1$ & $1$ & $w$ & $h$\\ \hline 
	$w$ & $1$ & $1$ & $x$ & $k$\\ \hline 	
\end{tabular}
\end{enumerate} 

\item 
\label{differentiation}
%AD is eqivalent to the chain rule, when the computer has definitions for functions and partials
%that can be adapted to match the compostition function.
It follows from \ref{composition} that we can evaluate $\frac{d}{d x}f(x) \mid_{x_0}$ with the chain rule: \\ \\
$\frac{d}{d x}f(x) = \frac{d}{d x} \cdot g(h(k(x))) \mid_{x_0}$ \\
$\frac{d}{d x}f(x) =  \frac{d{g}}{d{h}} \cdot \frac{d{h}}{d{k}}
\cdot \frac{d{k}}{d{x}}\mid_{x_0}$ \\ \\
From the rest of \ref{given} it follows that we can approximate  $\frac{d}{d x}f(x) \mid_{x_0}$ by
specializing the computer's general forms of $u, v, w$ 
according to \ref{abcvalues}, with parameters $a, b$ chosen for each function and held as constant, and
evaluating, such that the total derivative our original function w.r.t $x$ where $x = x_0$ is approximated
by the partial derivative of our equivalent statement, $f(x) = u(a, b, s) \mid_{arguments}$,  w.r.t $s$ 
where $s = x_0$. \\

\end{enumerate}
\label{doingthework}
%This shows how information flows through the nested functions in one expression, like a table of contents
For completeness we write out the computer's steps to evaluate \ref{differentiation} under the conditions of
\ref{abcvalues} with a particular value of $s = x_0$, in equation format: \\ \\
$\frac{\partial}{\partial x}f(x) \mid _{x_0} = \frac{\partial{u}}{\partial{v}} \mid _{1, 2, v \mid _{1, 1, w \mid _{ 1, 1, x_0}}} \cdot \frac{\partial{v}}{\partial{w}} \mid _{1,1, w \mid _{1, 1, x_0}} \cdot \frac{\partial{w}}{\partial{s}} \mid _{1, 1, x_0} \cdot \frac{ds}{dx}$\\ \\
Repeating above in tabular format: \\ \\
%This shows the same information but is ordered to show progression, like chapters
\begin{tabular}{l l l}
Current Evaluation & $s$ Value & Partial Derivative \\ \hline
$w(a, b, s)\mid_{1, 1, s}$ & $x_0$ & $\frac{\partial{w}}{\partial{s}} \mid _{1, 1, x_0}$ \\
$v(a, b, s)\mid_{1, 1, s}$ & $w(a, b, s)\mid_{1, 1, x_0}$ & $\frac{\partial{v}}{\partial{w}} \mid _{1,1, w \mid _{1, 1, x_0}} \cdot \frac{\partial{w}}{\partial{s}} \mid _{1, 1, x_0}$ \\
$u(a, b, s)\mid_{1, 2, s}$ & $v(a, b, s)\mid_{1, 1, w(a, b, s)\mid_{1, 1, x_0}}$ & $\frac{\partial{u}}{\partial{v}} \mid _{1, 2, v \mid _{1, 1, w \mid _{ 1, 1, x_0}}} \cdot \frac{\partial{v}}{\partial{w}} \mid _{1,1, w \mid _{1, 1, x_0}} \cdot \frac{\partial{w}}{\partial{s}} \mid _{1, 1, x_0} \cdot 1$ \\
\end{tabular} \\ \\
%This explains what the equation and table above mean.
Because the computer knew the analytical forms of the partial derivatives of each of $u,v,w$ beforehand,
all it needed to do was:
\begin{enumerate}
\item to evaluate each of $u,v,w$ according to \ref{abcvalues}, in order from $w \rightarrow
v \rightarrow u$, 
\item to remember the values for $x_0$ and the output of each function evaluation besides  $u$, 
\item then to use $x_0$ and the output of the function evaluations as input for the corresponding partial
derivative function  evaluations, and
\item store the individual partials.
\end{enumerate}
Lastly, to compute the partial derivative of the entire composition function, the computer multiplies
the individual partials together in observance of the chain-rule.  

Some things to note about AD are that no approximation of derivatives is being made because the analytical 
forms of the partials of the elementary math functions are defined alongside them. Accuracy of AD is then limited
by the precision of the AD system's definition of the elementary math functions and their partials.
 
\subsection{Tangent-stiffness Matrices} 

This subsection gives a working definition of a tangent-stiffness matrix and background on a
Newton-Raphson method which uses a tangent-stiffness matrix to help solve nonlinear systems of
equations, similarly to the the solution method used by the software package used in the study.

\subsubsection{What is a Tangent-stiffness Matrix}

It is important to identify what a tangent-stiffness matrix is and what it is used for to help show
the motivation for the work discussed here. It is helpful to begin with the thought that a
tangent-stiffness matrix is a type of slope for vector valued functions of vector variables at
a single point in the space of the vector valued variable. A tangent-stiffness matrix is similar to the
"$m$" in the scalar function $y = mx + b$, except that $y$ and $x$ are vectors while $m$ is
a matrix. \\

In detail, a tangent-stiffness matrix (or operator) can be described as a collection of the first
order partial derivatives of a vector valued function w.r.t each degree of freedom of the vector
valued function for a given value of the vector variable.  The tangent-stiffness matrix comprises a
linear operator that can be used to transform a difference in the vector variable into a difference
in the value of the vector valued function via an inner product between the tangent-stiffness matrix
and the vector variable.  As an example in a solid mechanics system, suppose some dependent
variables represented force components of force vectors at nodes of discretization, and independent
variables represented displacement components for those same nodes. Suppose that as is possible in
peridynamic nodes each dependent variable is a function of all of the independent variables. For
this situation, the tangent-stiffness matrix would be the first derivatives of every force component
variable with respect to every displacement component variable, with the rows ordering the force
components and the columns ordering the displacement components.  \\

How is a tangent-stiffness matrix computed? The mathematical formula for a tangent stiffness matrix
can be expressed in indicial notation as:\\\\ $\frac{\partial F_i}{\partial X_j}\mid_{X_0,...,X_n}$
\\\\ $F_i$ is the i'th component of the vector valued function, $X_j$ is the j'th component of the
vector variable, and a particular value of the vector variable is chosen $X_0,...,X_n$. One then
evaluates the expression for each combination $i, j$ corresponding to an element at $row, column$ in
the tangent-stiffness matrix. By inspection, the elements of a tangent-stiffness matrix can be
estimated using the CTSE, AD and finite-difference techniques for functions $F:R^1 \rightarrow R^1$,
since taking partial derivatives entails holding all but a single independent variable of the
function constant, and each component of a vector valued function can be evaluated independently of
the other components. \\

For reference, expressions for calculating tangent stiffness matrices with CTSE, AD,
central-difference and forward-difference will be shown and explained.

\subsubsection{Solve a Nonlinear System with a Tangent-stiffness Matrix} \label{solve_a_system}
\remove[Michael]{ 
An accurate tangent-stiffness matrix is a main part of the Newton-Raphson method for iteratively
solving non-linear systems for equilibrium solutions. The method is derived by a linearisation using
Taylor expansion of the vector valued function modelling the system in a way reminiscent of how
forward-difference is derived from the Taylor expansion of a scalar valued function. Here the two
derivations are shown side by side: The difference between the two is that in the forward difference
derivation, it is assumed that the derivative term is unknown and can be determined from knowing
change in location, while in the Newton-Raphson derivation, it is assumed that the change in
location is unknown and can be determined from knowing the value of the derivative term.
Additionally we could derive a version of Newton Raphson for CTSE directly: The Newton Raphson
method is applied in the following manner: It is clear to see how if the tangent-stiffness matrix
used in the Newton Raphson method is inaccurate, predicted changes in location will be inaccurate.
However, non-linear functions don't have constant slopes by definition, meaning that a perfectly
accurate tangent-stiffness matrix at the current guess location is not a guarantee of a one step
solution path to the solution location since slope changes over the interval guess to updated guess.
}
The reader is referred to a succinct explanation of the Newton Raphson method, which shows the
role of a tangent-stiffness matrix or Jacobian in that solution method \cite[chap.
13]{young2009}. 

\section{Methods and Materials} 
\subsection{Material: Perdynamics Code}
\subsubsection{\emph{Perdigm} and Peridynamics}
\subsubsection{Implementing Complex-Step in \emph{Peridigm}}

\subsection{The Comparative Study}

\subsubsection{Justification: Goals, Assumptions, and Metrics} 
A goal of the study was to rank complex-step against forward-difference and central difference on
the basis of accuracy. AD was ommited from the comparison because it served as the standard of
accuracy for the other methods in the absence of appropriate analaytical forms for the Jacobian
associated with the nonlinear system solved in the study. The assumption that AD is accurate
enough to serve as a standard is supported by AD's implementation as a computerized chain rule as
explained in \ref{ADsubsection}. For a concrete comparison of each of the methods including AD to a
known analytical Jacobian, refer to the corresponding author's website, specifically the C++
source-code example \emph{BeamAndSpringSystem}. 

Because it makes no sense to compare tangent-stiffness matrices from different problems, load steps,
or from different iterations, it was necessary to solve one load step and conclude each iteration
within that load step by updating displacement guess only from the the results of the AD method, and
to subsequently feed all four methods the same updated guess the following iteration. This decision
precluded a comparison of convergence rate, since if the methods were allowed to solve a problem at
their individual pace, differences in accuracy would produce differences in guess updates and
therefor the number and nature of iterations perfomed. Different guesses from iterations started
with different previous guesses could not be data for a valid comparison of tangent-stiffness
accuracy between methods.  However a separate convergence rate comparison study could be done given
some modifications to the materials for the study described in this paper, and of course, the
resources to perform that study.   Additionally, having each of the methods physically operate in
the same process, single or parallel, allowed the comparison of tangent-stiffness matrices as loaded
from random access memory rather than from the hard-disk.  Loading from RAM gives the benefit of
vastly greater speed and the simpler programming compared to some other solution speculatively
involving dynamic file management on files which for one of the components of the study would be on
the order of $1XE10$ bytes for which "vastly" is not hyperbole. However, the price of running the
methods together in the same process and avoiding the use of the hard-disk was that at least two
tangent-stiffness matrices had to be stored in RAM during simulation, which meant that special
high-memory compute nodes were needed for the $1E6$ peridynamic element problem series.

The other goal was to rank the four methods, that is including AD, on the basis of speed. Instead of
convergence rate being measured, speed of iteration was measured because it could be so done at the
same time as accuracy was being measured given a single tangent-stiffness calculation.  It was
assumed that the order that each method was evaluated was unimportant, that evaluating each method
sucessively within each solver iteration did not affect their performance individually and that
speed of computation did not change with time. These assumptions allowed the test program to run the
same problem with each of the methods at effectively the same time and generate an equal volume of
data from each method. It was also assumed that the Jacobian matrix calculation time for each of the
methods did not vary based on what values the independent variables held, as they do from iteration
to iteration as the guessed equilibrium solution is updated.  This assumption allowed calculation
time measurements to be averaged over all iterations within a solution attempt. The purpose of
averaging calculation time measurements was to informally address the extraneous variables of
evaluation order and computer system load due to other user's processes or processes not associated
with the study. It begs the question that if it is assumed that the value of independent variables
had no effect on iteration speed, than why couldn't each of the method have been run independently
so that convergence data were recorded, so long as system being solved was the same? The answer is
that this would have made accuracy comparisons invalid if measured concurrently with convergence
rate, while it would cost additional time and resources if convergence rate were measured in
serparate runs. Instead, while not part of the study, the example programs available on the
corresponding author's website allow the reader to make a comparison of the methods for themselves
on the basis of accuracy and convergence rate for two example nonlinear systems. In the examples,
notice how convergence rate decreases when the step size selected for forward-difference or
complex-step is set to larger numbers such that accuracy relative to AD is decreased. While likely
not the best possible implementations of these methods, they each were coded with the same skill
level such that a comparison of their relative strengths is reasonable from the standpoint of
intuition.

Once the selection of what to measure, why to measured it, and if it could be measured was
determined, the task was to find a way to take measurements that satisfied our goals. The goals of collecting accuracy and
speed measurements were achieved by developing and implementing metrics within the simulation
program used in the study. The metric used to measure the accuracy of a tangent-stiffness matrix was
the Frobenius norm of the element-wise difference between the tangent-stiffness matrix produced by
the method being evaluated and the tangent-stiffness matrix produced by the AD based method, given
that both methods were set upon the same problem. The lower the value of this metric, the closer the
other method's Jacobian was to the AD Jacobian, and therefore the more accurate the method was. The
expression for the accuracy metric:\\ $D = \sqrt{\sum_{i=0}^{n-1}\sum_{j=0}^{n-1} (J_{AD}(i,j) -
J_{M}(i,j))^2}$\\ Where $D$ is distance, n is the number of degrees of freedom, $J_{AD}$ is the
Jacobian matrix produced by the AD based method, M stands for other \emph{M}ethod, and the root of
the summed squared element-wise differences between the matrices is taken.  The metric used to
compare the speed of the methods was the time in seconds taken to resume execution in the calling
method after instructing the implementation of the method being evaluated to run and complete itself. Another basis of
comparison used in the study, but not quite a metric, called computational efficiency is based
on the ratio of calculation time to number of Jacobian matrix elements.

\subsubsection{Materials: Computers, Other Software}
Mesh generation for the test problems run in the study was done using the software package $Cubit$
developed at REFERENCE. 

\subsubsection{Methods: Reduction of Data}

\section{Results and Discussion}

\begin{center}
    \begin{tabular}{c c c c }
    \hline
    \multicolumn{4}{c}{Averaged Results, Entire Test Matrix} \\ \hline
    Cores & Jacobian Els. & Calc. Time (s) & Accuracy Diff. (MPa)\\ 
    \multicolumn{2}{c}{} & CS, CD, FD, AD & CS, CD, FD \\ \hline 
	1 & 1.99E6  & 3.5, 2.7, 1.7, 1.6 & 1.92E-10, 1.21E-4, .137 \\ \hline
	1 & 4.25E6  & 6.2, 4.9, 3.2, 2.9 & 2.28E-10, 9.94E-4, .148 \\ \hline
	1 & 7.79E6  & 11.2, 8.9, 5.7, 5.2 & 2.38E-10, 1.59E-4, .145\\ \hline
	1 & 1.54E7  & 26.7, 21.0, 13.4, 12.2 & 2.33E-10, 4.61E-4, .12 \\ \hline
	1 & 2.00E7  & 28.1, 22.2, 14.4, 13.1 & 2.76E-10, 1.05E-3, .145 \\ \hline
	1 & 3.14E7  & 47.6, 37.7, 24.2, 21.9 & 2.64E-10, 1.65E-3, .133 \\ \hline
	1 & 4.01E7  & 55.6, 44.1, 28.4, 25.9 & 3.03E-10, 1.92E-3, .148 \\ \hline
	1 & 8.39E7  & 138.9, 109.6, 70.2, 64.0 & 3.63E-10, 1.64E-3, .123 \\ \hline
	1 & 1.65E8  & 277.3, 218.1, 139.4, 126.5 & 3.26E-10, 2.18E-3, .128 \\ \hline
	32 & 1.67E10  & 336.1, 277.7, 200.6, 233.1 & 6.21E-10, 1.52E-2, .176 \\ \hline
	64 & 1.67E10  & 169.9, 140.7, 102.0, 119.7 & 6.20E-10, 1.50E-2, .177 \\ \hline
	96 & 1.67E10  & 114.7, 95.0, 69.1, 79.7 & 6.18E-10, 1.50E-2, .177 \\ \hline
	128 & 1.67E10  & 86.4, 71.8, 52.4, 58.8 & 6.16E-10, 1.47E-2, .177 \\ \hline
    \end{tabular}
\end{center}

\subsection{Speed Data}
\subsection{Accuracy Data}
\subsection{Efficiency Data}

\section{Conclusions and Further Work}
\subsection{Thoughts on Results}
\subsection{Thoughts on \emph{Perdigm}}
\subsection{Final Thoughts on Complex-Step}



%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
\appendix

\section{Extra}
\label{}

%% References
%%
%% Following citation commands can be used in the body text:
%% Usage of \cite is as follows:
%%   \cite{key}         ==>>  [#]
%%   \cite[chap. 2]{key} ==>> [#, chap. 2]
%%

%% References with bibTeX database:

\bibliographystyle{elsarticle-num}
\bibliography{all}

%% Authors are advised to submit their bibtex database files. They are
%% requested to list a bibtex style file in the manuscript if they do
%% not want to use elsarticle-num.bst.

\end{document}
%%
%% End of file `elsarticle-template-num.tex'.
