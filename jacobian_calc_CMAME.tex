%%
%% Copyright 2007, 2008, 2009 Elsevier Ltd
%%
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%%

%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%% SP 2008/03/01
%%
%%
%%
%% $Id: elsarticle-template-num.tex 4 2009-10-24 08:22:58Z rishi $
%%
%%
\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% if you use PostScript figures in your article
%% use the graphics package for simple commands
%%\usepackage{graphics}
%% or use the graphicx package for more complicated commands
%%\usepackage{graphicx}
%% or use the epsfig package if you prefer to use the old commands
%% \usepackage{epsfig}
%%\usepackage{color}
%%\usepackage{tkz-base}
\usepackage{pgfplots}
\usepackage{siunitx}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}

%Package for adding notes/comments/and tracking changes
\usepackage[inline]{trackchanges}
\addeditor{JTF}
\addeditor{Michael}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
%% \usepackage{lineno}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%
%% \biboptions{comma,round}

% \biboptions{}


\journal{Computer Methods in Applied Mechanics and Engineering}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
\author{Michael D. Brothers}
\author{John T. Foster\corref{cor1}}
\ead{john.foster@utsa.edu}
\cortext[cor1]{Corresponding Author} 

\author{Harry R. Millwater\corref{}}
\address{Mechanical Engineering Department, The University of Texas at San Antonio}

%% \ead{email address}
%% \ead[url]{home page}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\title{A comparison of different methods for calculating tangent-stiffness matrices in a massively parallel computational peridynamics code.}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \address[label1]{<address>}
%% \address[label2]{<address>}

\begin{abstract} %% Text of abstract Shown is a retrospective comparative study of tangent-stiffness
matrix calculation methods including a newly developed Complex Taylor Series Expansion
(CTSE)-based ``complex-step'' method alongside established methods: forward-difference,
central-difference and automatic differentiation.  Tangent-stiffness matrix is a term from
computational mechanics which here refers to the Jacobian matrix as used in first-order
Newton-Raphson methods for the solution of non-linear algebraic systems of equations developed
to describe physical or theoretical systems studied in math, science, engineering and business.
To perform the comparative study, the above tangent-stiffness calculation methods were applied
in a massively parallel computational peridynamics code developed at Sandia National Labs,
called \emph{Peridigm}. In the comparative study, for each datum for each run, complex-step was
multiple orders of magnitude more accurate the finite-difference methods, according to a common
comparison made to the automatic-differentiation method datum. However, for the implementation
contemporary to the study, complex-step was also the slowest method among the four for each
datum for each run. A mathematical definition of complex-step and
and automatic differentiation, description of the computer implementations and justification
of the comparative study precede results for clarity. The intended audience of this paper
includes researchers, professionals and students, therefore readers are directed to
selected background sources in the text for well-known concepts while less familiar concepts
are derived in-paper with an attempt made to lay steps out plainly for those in disparate fields
or who are new to their studies.  
\end{abstract}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword
Newton's method \sep Newton-Raphson \sep numeric differentiation \sep complex-step \sep finite-difference \sep automatic-differentiation \sep Jacobian \sep tangent-stiffness
%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)
%\MSC [2010] 65D25 
\end{keyword}

\end{frontmatter}

%%
%% Start line numbering here if you want
%%
% \linenumbers

%% main text


\section{Introduction}
%\subsection{Motivation}
\label{sec:intro} 

In order to maintain the quadratic convergence properties of the first-order Newton's method \cite{belytschko1999nonlinear} \cite[Ch.~13]{young2009} in quasi-static nonlinear analysis of solid structures it is crucial to obtain accurate, algorithmically consistent tangent-stiffness matrices. For an extremely small class of nonlinear material models, these consistent tangent-stiffness operators can be derived analytically; however, most often in practice, they are found through numerical approximation of derivatives. 

A goal of this study was to develop and evaluate a new, accurate, and practical method for calculating tangent-stiffness matrices against established methods.  This new method is based on a complex number Taylor series expansion and referred to as CTSE or the ``complex-step'' method.  The
distinction of `accurate' is defined by comparison of the new method to popular finite differencing techniques used for computing tangent-stiffness matrices and with the exact algorithmically consistent derivatives computed with \emph{automatic differentiation}.

Comparative data regarding the accuracy and compute cycle timing for the complex step, finite-difference, central-difference, and automatic differentiation methods are included to aide developers and analysts in computational mechanics code design who are faced with implementing a general method for tangent stiffness matrix calculation.  A comparison and discrimination of the methods was achieved through in-situ instrumentation of a massively parallel computational mechanics code.   

The scope of the application component of the study was limited to a single material model implemented in the computational peridynamics code, \textit{Peridigm} \cite{peridigm}. Identifying a specific application provided a practical framework for implementing the new method and solving engineering problems to generate the data needed to compare the methods. In particular, \emph{Peridigm} was chosen because it combined several helpful characteristics: the utilization of Newton's method and tangent-stiffness matrices for solving nonlinear quasi-static problems, prior inclusion of finite-difference, central-difference, and automatic-differentiation methods needed for comparison to the new complex-step method implementation, and being an agile-components code that makes use of distributed computing data structures via Trilinos \cite{trilinos} for efficient parallelization on large clusters.

The aim of this paper is not only to introduce the new complex-step method in the context of evaluating tangent-stiffness matrices, but to serve as a review for other differentiation techniques and as a general reference for solving non-linear systems with Newton's method. After presenting background information on the underlying methods to be discussed, presented in this paper are: a description of tangent-stiffness matrices and how to use them to solve non-linear systems, detailed directions for producing tangent-stiffness matrices with each the methods identified above, a description and justification of the new complex-step method for calculating tangent-stiffness matrices, a description of implementing complex-step in the \emph{Peridigm} software, a description of the quantities-of-interest used to rank the methods, a presentation and analysis of the results of the comparative study, and conclusions and thoughts on potential future work. Finally, in the interest of replicable research, the reader is referred to the corresponding author's website for C++ source-code and data necessary to reproduce the results presented here. Additionally, included on the website is a C++ library with classes for solving non-linear systems using the techniques discussed here, as well as two validation and verification example problems which show how to use the library.

\subsection{Differentiation Techniques}

This section contains background information on the differentiation techniques underlying the tangent-stiffness matrix calculation methods compared in the study. Since first-order finite-difference techniques are considered well known, they are not described here. Instead the reader is referred to \cite[Chap. 4.1.3]{chapra2010}.

\subsubsection{The ``complex-step'' method.}  
\label{sec:CSmethod}
It is possible to approximate derivatives quite accurately with a technique based on a complex variable Taylor series expansion of a function.  This method was first described by Lyness and Moler \cite{lyness1967numerical, lyness1968differentiation} and has more recently been rediscovered \cite{squire1998using, voorhees2011complex, al2010complex} for use in engineering analysis.  The basic idea is that a model parameter can be be made to be complex and expanded in a Taylor series about a small perturbation in the imaginary plane by some arbitrary value $h$ as follows
%
\begin{equation}
f ( x + i h ) = f (x) + \frac{\partial f(x)}{\partial x} \frac{i h}{1!} + \frac{\partial f^2(x)}{\partial x^2} \frac{(i h)^2}{2!} + \frac{\partial f^3(x)}{\partial x_n^3} \frac{(i h)^3}{3!} + \cdots
\label{eqn:complexTaylor}
\end{equation}
%
Now we can take the imaginary part of both sides of equation (\ref{eqn:complexTaylor}) and solve for the first derivative ignoring terms of $\mathcal{O}\left ( h^2 \right)$ to yield an the estimate
%
\begin{equation}
 \frac{\partial f( x )}{\partial x} \approx \frac{\mbox{Im} \left( f (x + i h) \right)}{h}.
\label{eqn:complexFirstDeriv}
\end{equation}
%
This shows that an estimate of the first derivative of a function can be made by only utilizing one functional evaluation of a perturbed model parameter up the imaginary axis.  The step-size $h$ is arbitrary and can be made as small as practical (even to machine precision without the dangers of roundoff error as in finite-differences) to yield an accurate estimate of this derivative.  The only disadvantage of this technique is the necessity of requiring the functions to accept complex numbers as arguments. Appendix~\ref{sec:appendixA} includes a simple example that illustrates the use of the complex-step method for derivative calculation of a function. 


\subsubsection{Automatic-differentiation} 
\label{ADsubsection}

Automatic-differentiation (AD) is a computerized method for computing exact derivatives based the chain-rule from calculus. AD takes advantage of the fact that any mathematical function executed on a computer, no matter how complicated, is a ``composition of simple operations'' (add, multiply, power, transcendental and the like) each having known analytical derivatives \cite{ref-sacado-presentation}. For reference, the AD implementation used in the study is part of the \emph{Sacado} package from the \emph{Trilinos} agile component libraries developed at Sandia National Laboratories \cite{ref-Sacado}.

The way an AD system works is by first evaluating the innermost function of the composition, then presenting that function's output as input to the next level function until all levels are complete, in a way no different from how a normal computer program or human would evaluate composition functions.  AD departs in that as each nested function is evaluated, the function's partial derivative with respect to the designated variables of the given input is also calculated.  This is possible because the elementary math functions are hard-coded into the AD source-code along with their analytical derivatives, and linked by special instructions, so that when the elementary math functions are called upon for computation, their partial derivatives may be computed and stored in a sequence. The AD system then multiplies the final sequence of partial derivatives together to produce the exact equivalent to taking a partial derivative of the corresponding composition function with respect to a designated variable at a particular value. It is obvious, but bears mentioning, that the AD system could simply store one value for partial derivatives, modifying it as appropriate for every function evaluation rather than keeping a sequence.  In the literature, the AD scheme described here is called \emph{forward automatic-differentiation}. Only forward AD will be covered here since it is the implementation used in \emph{Sacado} and therefore this study; however, the reader is referred to the introduction section of \cite{ref-AD-methods} and its citations for further information on AD, particularly \cite{ref-on-AD} which is foundational. Appendix~\ref{sec:appendixB} includes an example that walks the reader through the process a computer uses to compute derivates via AD.  Some things to note about AD are that no approximation of derivatives is being made because the analytical forms of the partials of the elementary math functions are defined alongside them. The accuracy of AD is then limited by the precision of the AD system's definition of the elementary math functions and their partial derivatives .
 
\subsection{Tangent-stiffness} 

In solid and structural computational mechanics the tangent-stiffness matrix is a linearization operator that describes the stiffness of a system in response to small displacements imposed upon the current configuration of the system.  Mathematically speaking, the tangent-stiffness represents the gradient of a high-dimensional energy surface that ``points'' in the minimum direction. While we've adopted the terminology of solid mechanics, it's important to note that these operators appear in other physical settings and are known by other names, e.g. a \emph{transmisability matrix} in the context of a Poisson problem or, more generally, a \emph{Jacobian matrix} in mathematical optimization.

While, for simplicity, the examples shown in the sequel refer only to linear problems, tangent-stiffness matrices generally arise in the context of non-linear analysis where Newton's method or quasi-Newton's method's are used in the minimization of some residual function.  In this context, the tangent-stiffness matrix can be thought of as the linearization of the system about about a particular configuration.  It has been shown \cite{hughes1978consistent, hughes1978unconditionally} that in order to preserve the quadratic convergence properties of the Newton's methods, it is necessary that this linearization is carried out in a manner that is \emph{consistent} with the algorithmic constraint equations used when computing the internal forces that arise due to deformations.  An example of these constraint equations would be the Kuhn-Tucker conditions \cite{simo1998} that are used in integration of a flow rule for plasticity modeling.  If the continuum tangent-moduli are used in place of the \emph{consistent} or \emph{algorithmic tangent moduli} in the solution of a non-linear solid mechanics problem, convergence may still be achieved, but not in the quadratic manner that makes Newton's method so attractive for this class of problems.  Again, in the setting of non-linear solid mechanics, there is a very small class of material model algorithms in which consistent tangent-stiffness operators can be derived analytically.  Perfect plasticity and plasticity with isotropic hardening used in combination with a general nearest-point projection or \emph{radial return algorithm} are a few examples.  Therefore, when these models (and certainly more complex ones) are implemented into a general purpose computational mechanics code, the tangent-stiffness operators are typically defined via a numerical approximation.

In detail, a tangent-stiffness matrix can be described as a collection of the first-order partial derivatives of a vector valued function with respect to each independent vector component of the function for a given vector.   A general mathematical formula for a tangent-stiffness matrix, $K_{ij}$,
can be expressed in indicial notation as 
%
\begin{equation} 
  K_{ij} = \left. \frac{\partial F_i}{\partial X_j}\right|_{\vec{X}_0},
\end{equation}
%
where $F_i$ is the $i^{\mbox{th}}$ component of the vector valued function, $X_j$ is the $j^{\mbox{th}}$ component of the vector argument to $F$, and a particular value of the vector is chosen to linearize about $\vec{X}_0$. One then evaluates the expression for each combination $(i, j)$ corresponding to a (row, column) location in the tangent-stiffness matrix. By inspection, the elements of a tangent-stiffness matrix can be estimated using any of the complex-step, AD, or common finite-difference techniques for functions $F:R^1 \rightarrow R^1$, since taking partial derivatives entails holding all but a single independent vector component of the function constant, and each element of the function can be evaluated independently of the others. \\ 

\subsubsection{Differencing formulas for tangent-stiffness matrix evaluation.}

The \emph{forward-difference} (FD) formula for calculating a tangent-stiffness matrix in the setting of a solid structural mechanics problem is
%
\begin{equation} 
  K_{ij} = \frac{F_i^{int}(\vec{u} + h \hat{e}_j) - F_i^{int}(\vec{u})}{h},
\end{equation}
%
Where $K_{ij}$ is the indicial notation representation of an element of the tangent-stiffness matrix at row $i$, column $j$. The first of the two terms in the numerator is the internal force, $F_i^{int}$ evaluated in the current deformed configuration, $\vec{u}$,  plus a small perturbation $h$ in the direction $\hat{e}_j$ where $\hat{e}_j$ represents a unit-vector corresponding to the $j^{\mbox{th}}$ degree-of-freedom. The second of the two terms is $F_i^{int}$ evaluated in the current configuration. The denominator is the magnitude of perturbation called \emph{probe-distance}.

The \emph{central-difference} (CD) formula for calculating a tangent-stiffness matrix is
%
\begin{equation} 
  K_{ij} = \frac{F_i^{int}(\vec{u} + h \hat{e}_j) - F_i^{int}(\vec{u} - h \hat{e}_j)}{2 h}.
\end{equation}
%
Together, the FD and CD methods, are sometimes termed \emph{finite-difference probing} techniques for tangent-stiffness calculation in literature \cite{ref-Adaggio}.

The complex-step (CS) formula for calculating a tangent-stiffness matrix is
%
\begin{equation} 
  K_{ij} = \frac{\mbox{Im}(F_i^{int}(\vec{u} + i h \hat{e}_j))}{h},
\end{equation}
%
where the internal force, $F^{int}$ is now treated as function of complex vectors,  $\vec{u}$. The small perturbation, $h$, is now carried out on the imaginary axis and only the real coefficient to the imaginary part of the vector returned by $F^{int}$ is kept, hence the $\mbox{Im}(\cdot)$ operation. As highlighted in Section~\ref{sec:CSmethod}, notice there is no subtraction operation taking place; therefore $h$ can be made very small to yield highly accurate derivatives.

When using automatic-differentiation to calculate a tangent-stiffness matrix, one only needs to follow the definition of the tangent-stiffness matrix and issue the correct commands to the AD system. The formula is the same as the continuum formula
%
\begin{equation} 
  K_{ij} = \frac{\partial F_i^{int}(\vec{u})}{\partial u_j},
\end{equation}
%
but, with the caveat that $F^{int}$ is evaluated in a way that is algorithmically consistent, and therefore should yield quadratic convergence with Newton's method.

\section{Description of analysis tools and approach.} 
%
\subsection{Peridigm and  peridynamics.}
%
Comparisons of the different tangent-stiffness calculation techniques were carried out through code-development and \emph{in situ} instrumentation of the computational peridynamics code \emph{Peridigm} \cite{peridigm}. \emph{Peridigm} is distributed by Sandia National Laboratories as its primary open-source computational peridynamics code. It is a massively-parallel simulation code for implicit and explicit multi-physics simulations primarily used for solid mechanics and material failure. \emph{Peridigm} is a C++ code utilizing agile software components from Sandia's  \emph{Trilinos} project \cite{trilinos}. It's important to note that there is no specialization of the tangent-stiffness calculation methods described previously to this particular code or more generally to a computational peridynamics approach and the analysis carried out in this study should provide insight into the speed and accruacy of these methods when implemented into other computational mechanics analysis tools as well.  \emph{Peridigm} was chosen because of the author's relationship with the code development project and primarily because the FD, CD, and AD methods for tangent-stiffness calculation where areleady implemented in the code, leaving only the CS method to for development. However, because peridynamics is a nonlocal theory, the tangent-stiffness matrices have a much higher bandwidth (i.e. less sparsity) than traditional finite-element tangent-stiffness computations and the time required to construct the tangent-stiffness is a majority of the total computation time in any quasi-static or implicit dynamics analysis; therefore, the \emph{Peridigm} development team has an interest in profiling the performance of the tangent-stiffness computation methods.

Briefly, peridynamics \cite{silling2000ret, silling:psa, silling2010peridynamic} is a nonlocal reformulation of the partial-differential equations that provide the statement of momentum balance in classical continuum mechanics. Its primary goal is the avoid the use of spatial derivatives in the balance equations or constitutive laws such that discontinuous displacements, i.e. cracks, are mathematically consistent with the governing equations. It has demonstrated great promise in modeling problems with massive pervasive failure \cite{littlewood2010}, interesting phenomenon such as dynamic crack branching \cite{ha2010sod}, and is being extended to multiphyics phenomenon \cite{bobaru2011peridynamic,katiyar2013} and coupling to molecular simulations \cite{seleson2009peridynamics}.

\subsubsection{Implementing the complex-step method in Peridigm} 
%
The CS method was implemented in \emph{Perdigm} in a manner that follows closely the implementation of the FD technique.  However, because of \emph{Peridigm}'s reliance on \emph{Trilinos} and specifically the \emph{Epetra} package, special steps had to be followed to allow the use of complex number data types.  This is because \emph{Epetra} vectors, which can be thought of as distributed memory parallel cousins to standard C++ STL vectors, are hard coded to be of type {\tt double} and can not be simply be declared as having a complex datatype. Therefore, program methods that were used in the course of applying the FD method were copied, renamed and re-written to follow the CS formula, which involved changing the datatype of intermediate variables which were locally scoped and dynamically casting persistent memory variables to complex datatypes where necessary. We made no atttempt to ``tune'' the CS code in any way for performance, we attempted to implement the algorithm is such as way that it was in one-to-one correspondence with the FD and CD methods; however, it must be noted that the overhead associated with the dynamic casts in the CS method likely hurt its overall performance and this issue could be alleviated by using the next generation templated \emph{TPetra} vectors available in \emph{Trilinos} that are capable of taking an explicit instantiation of any datatype including complex.  There where additional modifications for the purpose instrumenting the code to collect comparitive data and are not specifically part of CS or other methods. The source code patch to \emph{Peridigm} for implementing CS can be found on the corresponding author's website.  

\emph{Peridigm} was primarily written by expert computational scientists; therefore, many advanced techniques in C++ such as multiple virtual inheritance, pointer arithmetic, and distributed data structures were used in the basic code and by necessity in the modified code.  To see CS and other methods in a more narrow context, it is advised that readers also take a look at the simple examples provided on the corresponding author's website.

\subsection{The Comparative Study} 
\label{tcs}

The methods were compared by running two sets of test problems where each method would solve a problem concurrently. Each test problem simulated a $\SI{4}{\meter}$ by $\SI{0.25}{\meter}$ by $\SI{0.25}{\meter}$ meter block of a material undergoing tension along the axis parallel to the long dimension of the block. The test problems were set up in \emph{Perdigm} as quasistatic equillibrium problems, where the displacement boundary conditions used to apply tension were applied gradually, in `load steps', and an equilibrium solution for each load step was achieved before applying the next load step. The purpose of applying the load in gradual steps is a widely used technique to ensure the Newton's method is always initialized near the actual solution and therefore likely to converge. The material properties of the model were meant to replicate a typical metal and had values of $\SI{1.515e4}{\mega\pascal}$ for bulk modulus, and $\SI{7.813e4}{\mega\pascal}$ for shear modulus\note[JTF]{Micheal, are these numbers correct?}. The peridynamic horizon, a length scale that effectively sets the bandwidth of tangent-stiffness in the computation, used in the test problems was always set to three times the nominal discretization size (i.e. node spacing in the particle discretiztion scheme). This results in a minimum tangent-stiffness matrix bandwidth of 7 (for an opitimal node numbering scheme).

A series of nine single compute core (i.e. serial) test problems were run with the discretization level being refined with every test in the series. The aim of increasing the refinement was to see if possible differences between the methods in terms of accuracy and speed would arise. This series would comprise the single core runs. The specific parameters of these tests can be found in \emph{Peridigm} input {\tt xml} files included in the archive for this paper which can be found in the corresponding author's website. These {\tt xml} files allow users with appropriately modified versions of \emph{Peridigm} to reproduce the results found in this paper.

Another series of four test problems were simulated, however this time the number of compute cores used to solve the problem was increased from test to test while the discretization level was held constant at 1 million computational nodes (3 million degrees of freedom). This allowed for sufficient computational nodes per core even at the highest level of parallelization such that message passing compunication did not overwhelm the simulation.   The specific parameters of these tests can be found in \emph{Perdigm} input {\tt xml} files also included in the archive for this paper on the corresponding author's website.

\subsubsection{Quantities of interest} 
\label{JGAM} 
As stated in the Introduction, a goal of the study was to compare CS, FD, and CD on the basis of accuracy. AD was ommited from the comparison because it served as the standard of accuracy for the other methods in the absence of appropriate analaytical forms for the tangent-stiffness matrix associated with the system solved in the study. The assumption that AD is accurate enough to serve as a standard is supported by ADs implementation as a computerized chain rule as explained in Section~\ref{ADsubsection}.

It would be a poor comparative study to compare tangent-stiffness matrices from different problems, load steps that start with different current configurations, or from different iterations; because of this, it was necessary to solve one
load step and conclude each Newton iteration within that load step by updating the displacement iterate with only the results of the AD method and to subsequently feed all four methods the same updated displacement in the following iteration. This decision precluded a comparison of Newton iteration convergence rate, since if the methods were allowed to solve a problem at their individual pace, differences in accuracy would produce differences in guess updates and therefore the number and nature of Newton iterations performed (e.g. lower accuracy predictions of the algorithmically correct tangent-stiffness could cause a loss of quadratic convergence).  Different guesses from iterations started with different previous guesses could not be data for a valid comparison of tangent-stiffness accuracy between methods.  Additionally, having each of the methods physically operate in the same process, serial or one of associated parallel, allowed the comparison of tangent-stiffness matric calculation time as cached from random access memory rather than from the hard-disk.  Caching from RAM gives the benefit of vastly greater speed and the simpler programming compared to some other solution speculatively involving dynamic file management on files which for one of the components of the study would be on the order of 10 terabytes in size. However, the price of running the methods together in the same process and avoiding the use of the hard-disk was that at least two tangent-stiffness matrices had to be stored in RAM during the simulation, which meant that special high-memory compute nodes were needed for the 1 million peridynamic node test series.

The other goal was to compare the four methods, that is including AD, on the basis of speed. Instead of convergence rate being measured, speed of iteration was measured because it could be so done at the same time as accuracy was being measured given a single tangent-stiffness calculation.  It was assumed that the order that each method was evaluated was unimportant, that evaluating each method sucessively within each solver iteration did not affect their performance individually and that speed of computation did not change with time. These assumptions allowed the test program to run the
same problem with each of the methods at effectively the same time and generate an equal volume of data from each method. It was also assumed that the tangent-stiffness matrix calculation time for each of the methods did not vary based on what values the independent variables held, as they will from iteration to iteration as the Newton solution iterate is updated.  This assumption allowed calculation time measurements to be averaged over all iterations within a solution attempt. The purpose of averaging calculation time measurements was to informally address the extraneous variables of parallel evaluation order and computer system load due to system processes not associated with the study. While not part of the study, the example programs available on the corresponding author's website allow the reader to make a comparison of the methods for themselves on the basis of accuracy and convergence rate for two example nonlinear systems. In the examples, one will notice that the forward-difference method will lose quadratic convergence when the step size selected too large such that accuracy relative to AD is decreased.

Once the selection of what to measure was determined, the task was to find a way to take measurements
that satisfied our goals. The goals of collecting accuracy and speed measurements were achieved by
developing and implementing metrics within the simulation program used in the study. The metric used
to measure the accuracy of a tangent-stiffness matrix was the Frobenius norm of the element-wise
difference between the tangent-stiffness matrix produced by the method being evaluated and the
tangent-stiffness matrix produced by the AD based method, given that both methods were set upon the
same problem. The lower the value of this metric, the closer the other method's Jacobian was to the
AD Jacobian, and therefore the more accurate the method was. The expression for the accuracy metric:

\begin{equation} D = \sqrt{\sum_{i=0}^{n-1}\sum_{j=0}^{n-1} (J_{AD}(i,j) - J_{M}(i,j))^2}
\end{equation}

Where $D$ is distance, n is the number of degrees of freedom, $J_{AD}$ is the Jacobian matrix
produced by the AD based method, M stands for other \emph{M}ethod, and the root of the summed
squared element-wise differences between the matrices is taken.  The metric used to compare the
speed of the methods was the time in seconds taken to resume execution in the calling method after
instructing the implementation of the method being evaluated to run and complete itself. Another
basis of comparison used in the study, but not quite a metric, called computational efficiency is
based on the ratio of calculation time to number of Jacobian matrix elements.

\subsubsection{Materials: Computers, Other Software} 

Mesh generation for the test problems run in the study was done using the software package
\emph{Cubit} developed at Sandia National Labs \cite{ref-Cubit}. For the test series in which peridynamic node density was
varied, journal scripts compatible with \emph{Cubit} were used. The rule used to select the
arangement of nodes within the borders designated by simulated material block's dimensions was to
manually choose a node spacing, fit the maximum whole number of nodes in a regular grid within the
block, and then check whether or not this number of nodes matched the number of nodes desired for a
particular test. The same process was used for the multicore series. An example journal script can
be found on the corresponding author's website. For future work, it may be beneficial to automate
and randomize this process for time efficiency and to assist statistical analysis. 

The test problems were run the \emph{Stampede} HPC cluster computer housed at \emph{TACC} (Texas
Advanced Computing Center) based out of UT Austin. The single core test series was run using the
normal queue compute nodes, while the multicore test series was run using one to four "large memory"
nodes each having one terrabyte of local RAM. This large amount of memory was required for reasons
explained in \ref{JGAM}.  

\subsubsection{Methods: Reduction of Data} The output produced by \emph{Perdigm} including the
additional accuracy and speed measurements it was modified to provide was redirected from the
console to text files by the resource manager, \emph{SLURM} (Simple Linux User Resource Manager).
Directories and filenames were chosen so that data would be easy to sort by individual test run.
Extraneous information was manually deleted from copied versions of the output text files so that
they would be regular enough to be easily read by scripts written in \emph{Python}. These scripts
averaged accuracy or speed data for a test corresponding to a particular peridynamic node density
over all iterations for that run, and then plotted this averaged data as if it were a function of
peridynamic node density as it varied from test to test in the series.  Similarly, for the multicore
tests, data was averaged and plotted in much the same way, but as a funciton of number of cores used
to solve the problem rather than as a function peridynamic node density which was held constant.
These data reduction and plotting scripts can be found on the corresponding autor's website.

Since samples were not repeated, because there was not resources for it, confidence intervals were
not calculated. However, if results of iterations within the same test problem were considered
samples of the same thing, then a confidence interval could be calculated. This assumption was only made
for the purpose of reporting average values on the plots. But, a cursory review of data showed no 
range overlap among the methods for accuracy or speed data from any test run of either series.

\section{Results and Discussion}

This section contains the averaged results from the study presented in tabular and graphic format.
Table \ref{tab:results} shows the core count, number of nonzero Jacobian matrix elements, load step
average calculation time and load step average accuracy. The units for accuracy are derived from the
expression used to calculate the Jacobian matrix elements and the units used to define bulk material
properties, mentioned in \ref{tcs}.

\begin{table}    
        \caption{Averaged Results, Each Test} \label{tab:results}   
    \begin{tabular}{c c c c }
    Cores & Jacobian Els. & Calc. Time (s) & Accuracy Diff. (MPa)\\ 
    \multicolumn{2}{c}{} & CS, CD, FD, AD & CS, CD, FD \\ \hline 
	1 & 1.99E6  & 3.5, 2.7, 1.7, 1.6 & 1.92E-10, 1.21E-4, .137 \\ \hline
	1 & 4.25E6  & 6.2, 4.9, 3.2, 2.9 & 2.28E-10, 9.94E-4, .148 \\ \hline
	1 & 7.79E6  & 11.2, 8.9, 5.7, 5.2 & 2.38E-10, 1.59E-4, .145\\ \hline
	1 & 1.54E7  & 26.7, 21.0, 13.4, 12.2 & 2.33E-10, 4.61E-4, .12 \\ \hline
	1 & 2.00E7  & 28.1, 22.2, 14.4, 13.1 & 2.76E-10, 1.05E-3, .145 \\ \hline
	1 & 3.14E7  & 47.6, 37.7, 24.2, 21.9 & 2.64E-10, 1.65E-3, .133 \\ \hline
	1 & 4.01E7  & 55.6, 44.1, 28.4, 25.9 & 3.03E-10, 1.92E-3, .148 \\ \hline
	1 & 8.39E7  & 138.9, 109.6, 70.2, 64.0 & 3.63E-10, 1.64E-3, .123 \\ \hline
	1 & 1.65E8  & 277.3, 218.1, 139.4, 126.5 & 3.26E-10, 2.18E-3, .128 \\ \hline
	32 & 1.67E10  & 336.1, 277.7, 200.6, 233.1 & 6.21E-10, 1.52E-2, .176 \\ \hline
	64 & 1.67E10  & 169.9, 140.7, 102.0, 119.7 & 6.20E-10, 1.50E-2, .177 \\ \hline
	96 & 1.67E10  & 114.7, 95.0, 69.1, 79.7 & 6.18E-10, 1.50E-2, .177 \\ \hline
	128 & 1.67E10  & 86.4, 71.8, 52.4, 58.8 & 6.16E-10, 1.47E-2, .177 \\ \hline
    \end{tabular}
\end{table}



The data presented in \ref{tab:results} will now be shown in graphic form alongside some discussion
of results and trends.

\subsection{Speed Data}
Speed measurements were taken according to the methods mentioned in \ref{tcs}. The averaged results
of these speed measurments for the single core tests can be seen in \ref{fig:serial_speed}.
Average time in seconds to compute a Jacobian matrix element is plotted against number of nonzero
Jacobian matrix ekements.
Complex-step turned out to be the slowest of the four methods, taking the longest average time per
iteration. The next slowest method was central-difference, followed by forward-difference and
automatic-differentiation. Because the x axis of \ref{fig:serial_speed} is on a log scale, it
bears mentioning that while visually it may appear that the trend indicated by the progression of
datapoints on the plot is non-linear, the relationship would be closer to linear. This bodes well
for the scalability of each of the methods by themselves in \emph{Peridigm}. 

\begin{figure}[tbp]
  \centering
  \scalebox{1.0}{\input{./figs/serial_speed.pgf}}
  \caption{Serial test series speed measurements.}
  \label{fig:serial_speed}
\end{figure}

For the multicore tests, averaged speed results appear in \ref{fig:multi_speed}. For this series,
rather than varying the number of nonzero Jacobian matrix elements the number of cores used to
solve the problem is varied, while the number of nonzero Jacobian matrix elements remained constant.
Complex-step again was the slowest method, followed by central-difference, automatic-differentiation
and then forward-difference, taking the title of fastest method for this group of tests.
\ref{fig:multi_speed} indicates that when looking at average in-iteration speed, each of the
methods scale linearly with the number of cores used for the given conditions of this group of
tests. Again, these results show that each of the methods as implemented in \emph{Peridigm} scale
well when additional processors are used. An explanation as to why automatic-differentiation and
forward-differences swapped places in speed ranking relative to the single core results was not
determined. 

\begin{figure}[tbp]
  \centering
  \scalebox{1.0}{\input{./figs/multi_speed.pgf}}
  \caption{Multicore test series speed measurements.}
  \label{fig:multi_speed}
\end{figure}

\subsection{Accuracy Data}
Accuracy measurements where taken according to the methods mentioned in \ref{tcs}. The averaged
results of these accuracy measurements for the single core tests can be seen in
\ref{fig:serial_accuracy}. Averaged Frobenius norm of the element-wise difference between a given
Jacobian and the Jacobian produced by the automatic-differentiation method is plotted against number
of nonzero Jacobian matrix elements. Complex-step was revealed as the most accurate of the methods
besides automatic-differentiation, complex-step resulting in several orders of magnitude less
difference than resulted from using the central-difference method and the most inaccurate
forward-difference method. This accuracy ranking order was mantained for each test in the series. It
should be pointed out compared to the L2 norms of any of the Jacobian matrices by themselves, the
magnitude of these differences for any of the methods is relatively small. These values appear in
the experimental data which is available on the corresponding author's website. The effect of these
accuracy differences on convergence rate directly was not determined.

\begin{figure}[tbp]
  \centering
  \scalebox{1.0}{\input{./figs/serial_accuracy.pgf}}
  \caption{Serial test series accuracy measurements.}
  \label{fig:serial_accuracy}
\end{figure}

The averaged resuts of the accuracy measurements taken for the multicore test series can be seen in
\ref{fig:multi_accuracy}. Y axis units remain unchanged from \ref{fig:serial_accuracy}, but the x
axis now indicated number of cores used to solve the test problem. Accuracy trends matched those for
the single core tests. 

\begin{figure}[tbp]
  \centering
  \scalebox{1.0}{\input{./figs/multi_accuracy.pgf}}
  \caption{Multicore test series accuracy measurements.}
  \label{fig:multi_accuracy}
\end{figure}

\subsection{Efficiency Data}
A derived measurement was also used to compare the methods. The single core results are shown in
\ref{fig:serial_efficiency}. Efficiency, calculated as the average number of seconds taken per nonzero Jacobian matrix element 
was ploted against the number nonzero Jacobian matrix elements. The ranking order of the methods
matched their speed ranking in \ref{fig:serial_speed}. This plot indicates that there is a small,
yet present decrease in efficiency w.r.t number of nonzero Jacobian elements. But judging by the
speed results, this trend in efficiency doesn't appear to have great effect in human terms for the
mesh densities used in this test series.  

\begin{figure}[tbp]
  \centering
  \scalebox{1.0}{\input{./figs/serial_speed_rel.pgf}}
  \caption{Serial test series efficiency measurements.}
  \label{fig:serial_efficiency}
\end{figure}

The multicore efficiency results are shown in \ref{fig:multi_efficiency}. Y axis is average number
of seconds taken per nonzero Jacobian matrix element times number of cores used to solve the
problem. The x axis in number of cores used to solve the problem. The multicore efficiency plot
shows that automatic-differentiation was slower than forward-difference because for the given test
problem it had a lower efficiency. Strangely automatic-differentiation appeared to gain in
efficiency over the last few datapoints in the plot, looking at the raw data confirmed that this was
what was measured rather than this being an error during data reduction. The reason behind this
trend was not determined.

\begin{figure}[tbp]
  \centering
  \scalebox{1.0}{\input{./figs/multi_speed_rel.pgf}}
  \caption{Multicore test series efficiency measurements.}
  \label{fig:multi_efficiency}
\end{figure}

\section{Conclusions and Further Work}

\subsection{Thoughts on Results} Based on the experimental results, it appears that for users of
\emph{Peridigm} there is a case to be made for using automatic-differentiation for reasons of
performance and accuracy. Although, the greater speed of finite-difference for certain multicore
tests done in this study may give some users pause. While the results showed that complex-step
produced more accurate tangent-stiffness matrices than central-difference and forward-difference 
under the parameters of the tests, it was not determined whether or not this is a clear advantage of complex-step over
finite-difference in terms of accuracy of final predicted node positions and speed of convergence. For other
simulation software it is obviously necessary to collect the same types measurements as decribed in
this paper in order to make a design decision. For new developments it is recommended that combined
accuracy and speed of iteration mesurments be conduted along with entirely separate convergence
studies. 

\subsection{Final Thoughts on Complex-Step} Complex-step was shown to be highly accurate in
calculating tangent-stiffness matrices by the standards of this study, but as it is implemented in
\emph{Peridigm} may not be any more accurate than AD, and it is almost certainly slower for a naive
implementation. When looking to the future of \emph{Peridigm}, Complex-step holds the advantage of
relying on byte-copiable datatypes, while AD requires a complicated serialization and
deserialization in order to function as byte-copiable. This is fine for copying in between MPI
processes, but deserialization on an accelerator or GPU is very difficult, as this would require
compiling a version of the AD library being used that were compatible with the accelerator or GPU
being used. What this means is that if \emph{Peridigm} or any simulation software were to be
reformulated to take advantage of accelerators or GPUs, attractive given their performance and power
efficiency advantages over traditional CPUs, the flexibilty of complex-step owed to its simple
implementation may make it more viable for that application.



%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections

\section{Acknowledgements}
This work was funded in part by grants from the United States Air Force Office of Scientific
Research grant number XXXX and National Energy Technology Laboratory grant number XXXXX. 
\label{sec:ack}
%% References
%%
%% Following citation commands can be used in the body text:
%% Usage of \cite is as follows:
%%   \cite{key}         ==>>  [#]
%%   \cite[chap. 2]{key} ==>> [#, chap. 2]
%%

%% References with bibTeX database:

\bibliographystyle{elsarticle-num}
\bibliography{all}

%% Authors are advised to submit their bibtex database files. They are
%% requested to list a bibtex style file in the manuscript if they do
%% not want to use elsarticle-num.bst.
\appendix
\renewcommand*{\thesection}{\Alph{section}}
%
\section{Complex-Step Example}
\label{sec:appendixA}
%
Using $f(x) = \cos(x)$, we have
%
\[
f (x + i h) = \cos(x + i h),
\]
%
or through trigonometric identities equivalently
%
\[
f(x + i h) = \cos(x) \cosh(h) - i \sin(x) \sinh(h),
\]
%
Applying equation (\ref{eqn:complexFirstDeriv})
\[
\frac{\partial f(x)}{\partial x} = - \frac{\sin(x) \sinh(h)}{h},
\]
Since $h$ is arbitrary we desire it to be as small as possible or
\[
\frac{\partial f(x)}{\partial x} = \lim_{h \to 0} - \frac{\sin(x) \sinh(h)}{h},
\]
using L'H\^opital's rule
\[
\frac{\partial f(x)}{\partial x} = \lim_{h \to 0} - \frac{\sin(x) \cosh(h)}{1},
\]
of course, as $h \to 0$ then $\cosh(h) \to 1$ thereby recovering the exact derivative
\[
\frac{\partial f(x)}{\partial x} =-\sin(x).
\]

\section{AD Example}
\label{sec:appendixB}

\begin{enumerate}
%Set up example problem. The example is meant to walk the reader through the forward AD algorithm
%while encouraging them to rely on their understanding of the chain rule
\item Take an example composition function $f(x) = (sin(cos(x)))^2$\\

\item Suppose we want to know $\frac{df}{dx} \mid_{x_0}$ where $x_0$ is a particular value of $x$. \\

\begin{enumerate}
%This first block of subitems is meant to establish that the values of x that the user cares about
%about evaluating functions for are included within the set of numbers that functions g,h,k are allowed 
%to deal with, and that nested functions can take each other as input, where s may not necessarily equal
%x.
\item Given that: \\
\label{given}
\begin{enumerate}
\item $S \in R^1$ 
\item $X \in S$ 
\item $g, h, k \in H: S \rightarrow S$ 
\item $\forall s \in S : g(s) = {s}^2$, $h(s) = sin(s)$, $k(s) = cos(s)$ 
\item
\label{composition}
$\forall x \in X : f(x) = g(h(k(x)))$ \\
\end{enumerate}

\item Given that the computer is programmed with some mathematical definitions:\\
\begin{enumerate}
\item
\begin{enumerate}
\item
%The computer has at least definitions for these functions and variables
$u, v, w \in H: R^3 \rightarrow R^1$ \\
\item
$s,a,b,x \in R^1$\\
\end{enumerate}

\item
%AD doesn't return symbolic epressions for derivitives that can be re-evaluated.
%Instead, generic versions of functions and evaluations of their partials are defined together.
particular values can be identified: \\
$s = s_0,..,s_n,...s_{\infty} \mid n=[0,\infty)$ \\ 
and similarly for the other variables $a,b,x$ \\
\item
functions $u,v,w$ and their partial derivatives w.r.t $s$ are defined such that: \\
\begin{tabular}{l | l}
\multicolumn{2}{c}{for $a, b, s$ equal to $a_n, b_n, s_n$:} \\ \hline
$u \mid _{a_n, b_n, s_n} = a_n \cdot s_n^{b_n}$ & $\frac{\partial{u}}{\partial{s}} \mid _{a_n, b_n, s_n} = a_n \cdot b_n \cdot s_n^{b_n - 1}$ \\
$v \mid _{a_n, b_n, s_n} = a_n \cdot sin(b_n \cdot s_n)$ & $\frac{\partial{v}}{\partial{s}} \mid _{a_n, b_n, s_n} = a_n \cdot b_n \cdot cos(b_n \cdot s_n)$\\ 
$w \mid _{a_n, b_n, s_n} = a_n \cdot cos(b_n \cdot s_n)$ & $\frac{\partial{w}}{\partial{s}} \mid _{a_n, b_n, s_n} = -a_n \cdot b_n \cdot sin(b_n \cdot s_n)$\\
\end{tabular} 
\end{enumerate}
%a comment
\item 
\label{abcvalues}
%General definitions of functions are specialized by arguments. 
Given that it is possible to describe $f(x) = (sin(cos(x)))^2$ in terms the computer understands by inputting
$f(x)$ such that the computer stores an equivalent statement $f(x) = u(a, b, s) \mid_{arguments}$,
iff the arguments of $u,v,w$ are chosen such that $u,v,w$ approximate $g,h,k$ as follows:\\
\begin{tabular}{l l l l | c}
	Function & a & b & s & Approximates Function\\ \hline 
	$u$ & $1$ & $2$ & $v$ & $g$\\ \hline 
	$v$ & $1$ & $1$ & $w$ & $h$\\ \hline 
	$w$ & $1$ & $1$ & $x$ & $k$\\ \hline 	
\end{tabular}
\end{enumerate} 

\item 
\label{differentiation}
%AD is eqivalent to the chain rule, when the computer has definitions for functions and partials
%that can be adapted to match the compostition function.
It follows from \ref{composition} that we can evaluate $\frac{d}{d x}f(x) \mid_{x_0}$ with the chain rule: \\ \\
$\frac{d}{d x}f(x) = \frac{d}{d x} \cdot g(h(k(x))) \mid_{x_0}$ \\
$\frac{d}{d x}f(x) =  \frac{d{g}}{d{h}} \cdot \frac{d{h}}{d{k}}
\cdot \frac{d{k}}{d{x}}\mid_{x_0}$ \\ \\
From the rest of \ref{given} it follows that we can approximate  $\frac{d}{d x}f(x) \mid_{x_0}$ by
specializing the computer's general forms of $u, v, w$ 
according to \ref{abcvalues}, with parameters $a, b$ chosen for each function and held as constant, and
evaluating, such that the total derivative our original function w.r.t $x$ where $x = x_0$ is approximated
by the partial derivative of our equivalent statement, $f(x) = u(a, b, s) \mid_{arguments}$,  w.r.t $s$ 
where $s = x_0$. \\

\end{enumerate}
\label{doingthework}
%This shows how information flows through the nested functions in one expression, like a table of contents
For completeness we write out the computer's steps to evaluate \ref{differentiation} under the conditions of
\ref{abcvalues} with a particular value of $s = x_0$, in equation format: \\ \\
$\frac{\partial}{\partial x}f(x) \mid _{x_0} = \frac{\partial{u}}{\partial{v}} \mid _{1, 2, v \mid _{1, 1, w \mid _{ 1, 1, x_0}}} \cdot \frac{\partial{v}}{\partial{w}} \mid _{1,1, w \mid _{1, 1, x_0}} \cdot \frac{\partial{w}}{\partial{s}} \mid _{1, 1, x_0} \cdot \frac{ds}{dx}$\\ \\
Repeating above in tabular format: \\ \\
%This shows the same information but is ordered to show progression, like chapters
\begin{tabular}{l l l}
Current Evaluation & $s$ Value & Partial Derivative \\ \hline
$w(a, b, s)\mid_{1, 1, s}$ & $x_0$ & $\frac{\partial{w}}{\partial{s}} \mid _{1, 1, x_0}$ \\
$v(a, b, s)\mid_{1, 1, s}$ & $w(a, b, s)\mid_{1, 1, x_0}$ & $\frac{\partial{v}}{\partial{w}} \mid _{1,1, w \mid _{1, 1, x_0}} \cdot \frac{\partial{w}}{\partial{s}} \mid _{1, 1, x_0}$ \\
$u(a, b, s)\mid_{1, 2, s}$ & $v(a, b, s)\mid_{1, 1, w(a, b, s)\mid_{1, 1, x_0}}$ & $\frac{\partial{u}}{\partial{v}} \mid _{1, 2, v \mid _{1, 1, w \mid _{ 1, 1, x_0}}} \cdot \frac{\partial{v}}{\partial{w}} \mid _{1,1, w \mid _{1, 1, x_0}} \cdot \frac{\partial{w}}{\partial{s}} \mid _{1, 1, x_0} \cdot 1$ \\
\end{tabular} \\ \\
%This explains what the equation and table above mean.
Because the computer knew the analytical forms of the partial derivatives of each of $u,v,w$ beforehand,
all it needed to do was:
\begin{enumerate}
\item to evaluate each of $u,v,w$ according to \ref{abcvalues}, in order from $w \rightarrow
v \rightarrow u$, 
\item to remember the values for $x_0$ and the output of each function evaluation besides  $u$, 
\item then to use $x_0$ and the output of the function evaluations as input for the corresponding partial
derivative function  evaluations, and
\item store the individual partials.
\end{enumerate}
Lastly, to compute the partial derivative of the entire composition function, the computer multiplies
the individual partials together in observance of the chain-rule.  

\end{document}
%%
%% End of file `elsarticle-template-num.tex'.
