\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% if you use PostScript figures in your article
%% use the graphics package for simple commands
%%\usepackage{graphics}
%% or use the graphicx package for more complicated commands
%%\usepackage{graphicx}
%% or use the epsfig package if you prefer to use the old commands
%% \usepackage{epsfig}
%%\usepackage{color}
%%\usepackage{tkz-base}
\usepackage{pgfplots}
\pgfplotsset{compat=1.8}
\usepackage{siunitx}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{array}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage[T1]{fontenc}
\usepackage{upquote}
%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}

%Package for adding notes/comments/and tracking changes
\usepackage[disable, color=red!40, textsize=scriptsize]{todonotes}

%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
%% \usepackage{lineno}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%
%% \biboptions{comma,round}

% \biboptions{}


\journal{Computer Methods in Applied Mechanics and Engineering}

\begin{document}\sloppy

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \addedress for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
\author{Michael D. Brothers}
\author{John T. Foster\corref{cor1}}
\ead{john.foster@utsa.edu}
\cortext[cor1]{Corresponding Author} 
\author{Harry R. Millwater\corref{}}
\address{Mechanical Engineering Department, The University of Texas at San Antonio}

%% \ead{email address}
%% \ead[url]{home page}
%% \addedress{Address\fnref{label3}}
%% \fntext[label3]{}

\title{A comparison of different methods for calculating tangent-stiffness matrices in a massively parallel computational peridynamics code.}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \addedress[label1]{<address>}
%% \addedress[label2]{<address>}

\begin{abstract} %% Text of abstract Shown is a retrospective comparative study of tangent-stiffness

    In order to maintain the quadratic convergence properties of Newton's method in quasi-static nonlinear analysis of solid structures it is crucial to obtain accurate, algorithmically consistent tangent-stiffness matrices. A goal of the study described in this paper was to establish the suitability of an underexplored method for numerical computation of tangent-stiffness operators, referred to as ``complex-step'', and compare the  method with other techniques for numerical derivative calculation: automatic differentiation, forward finite difference, and central finite difference. The complex-step method was implemented in a massively parallel computational peridynamics code for the purpose of this comparison.  The methods were compared through profiling of the code for accuracy, speed, efficiency, and parallel scalability. This research provides data that can serve as practical guide for code developers and analysts faced with choosing which method best suits the needs of their application code.  Additionally, motivated by the reproducible research movement, all of the code, examples, and workflow to regenerate the data and figures in this paper are provided as open source.


\end{abstract}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword
Newton's method \sep Newton-Raphson \sep numerical differentiation \sep complex-step \sep finite difference \sep automatic differentiation \sep tangent-stiffness
%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)
%\MSC [2010] 65D25 
\end{keyword}

\end{frontmatter}

%%
%% Start line numbering here if you want
%%
% \linenumbers

%% main text

\section{Introduction.}
%\subsection{Motivation}
\label{sec:intro} 

In order to maintain the quadratic convergence properties of the first-order Newton's method \cite{belytschko1999nonlinear} \cite[Ch.~13]{young2009} in quasi-static nonlinear analysis of solid structures it is crucial to obtain accurate, algorithmically consistent\todo[]{Re: II.1}\footnote{In the context of plasticity theory, the algorithmic consistency of the tangent-stiffness matrix refers to consistency with the integration of a plasticity model under the Khun-Tucker constraints or similar where the tangent-stiffness matrix is computed using the elastoplastic modulii that are determined from the integration \cite{simo1998}.} tangent-stiffness matrices. For an extremely small class of nonlinear material models, these consistent tangent-stiffness operators can be derived analytically; however, most often in practice, they are found through numerical approximation of derivatives. 

A goal of this study was to develop and evaluate a \todo[]{Re: I.1} relatively unexplored, accurate, and practical method for calculating tangent-stiffness matrices against established methods.  This  \todo[]{Re: I.1} method is based on a complex number Taylor series expansion and referred to as the \emph{complex-step} (CS) method. The practical accuracy of CS is determined by comparing its measured accuracy relative to the measured accuracy of popular finite differencing techniques used for computing tangent-stiffness matrices. For this comparison the,  exact to machine precision, derivatives computed with \emph{automatic differentiation} serve as the standard.

Comparative data regarding the accuracy and compute cycle timing for the complex step, forward difference, central difference, and automatic differentiation methods are included to aid developers and analysts in computational mechanics code design who are faced with implementing a general method for tangent-stiffness matrix calculation.  A comparison of the methods was achieved through instrumentation of a massively-parallel computational mechanics code. 

The scope of the application component of this study was limited to a single material model, an elastic peridynamic solid, implemented in the computational peridynamics code, \textit{Peridigm} \cite{peridigm}. Identifying a specific application provided a practical framework for implementing the  \todo[]{Re: I.1} complex-step method and solving engineering problems to generate the data needed to compare the methods. In particular, \emph{Peridigm} was chosen because it combined several helpful characteristics: the utilization of Newton's method and tangent-stiffness matrices for solving nonlinear quasi-static problems, prior inclusion of forward difference, central difference, and automatic-differentiation methods needed for comparison to the complex-step method implementation, and being an agile-components code that makes use of distributed computing data structures via Trilinos \cite{trilinos} for efficient parallelization on large clusters. Agile-components is a software development term meaning to structure a software project for continuous development to respond to evolving specifications by making much use of techniques from object-oriented programming and reusing proven pre-existing code to reduce development time and increase reliability. The modularity of \emph{Peridigm} allowed the adding of new features, such as those required by this study.

The aim of this paper is not only to \todo[]{Re: I.1} study the complex-step method in the context of evaluating tangent-stiffness matrices, but to serve as a review for other differentiation techniques useful for solving nonlinear systems with Newton's method. After presenting background information on the underlying methods to be discussed, presented in this paper are: In Section~\ref{subsec:TS} a description of tangent-stiffness matrices, detailed directions for producing tangent-stiffness matrices with each of the methods identified above, and a description and justification of the  \todo[]{Re: I.1} complex-step method for calculating tangent-stiffness matrices. Section~\ref{sec:Validation} provides a simple illustrative example for using each of the methods for calculating tangent-stiffness matrices in the solution of nonlinear problems. In Section~\ref{subsubsec:Impl} a description of implementing the complex-step method in the \emph{Peridigm} software. In Section~\ref{JGAM} a description of the quantities-of-interest used to rank the methods. In Sections~\ref{sec:Res} and \ref{sec:PeridigmConvergenceStudy} a presentation and analysis of the results of the comparative study, and in Section~\ref{sec:Conc} conclusions and thoughts on potential future work.  Finally, in the interest of replicable research, the reader is referred to the corresponding author's website for \emph{Peridigm} C++ source-code and data necessary to reproduce the results presented here. Additionally, included on the website is an example serial C++ library with classes for solving nonlinear systems using the techniques discussed here, as well as two validation and verification example problems which show how to use the library.  The purpose of the serial C++ library is to demonstrate in a less complex manner than in the \emph{Peridigm} code how one would use automatic-differentiation or complex-step to better encourage their use and discussion.  \todo[]{Re: II.5}This open source resource is available at the corresponding author's website or at \href{https://github.com/utsa-idl/Complex_Step_Jacobian_Project_Archive}{https://github.com/utsa-idl/Complex\_Step\_Jacobian\_Project\_Archive}.

\subsection{Differentiation techniques}

This section contains background information on the differentiation techniques underlying the tangent-stiffness matrix calculation methods compared in this study. Since the derivation of first-order finite difference techniques from Taylor series expansion is considered well-known, it is not described here.  Instead the reader is referred to \cite[Chap. 4.1.3]{chapra2010}.

\subsubsection{The ``complex-step'' method}  
\label{sec:CSmethod} 

It is possible to approximate derivatives quite accurately with a technique based on a complex-variable Taylor series expansion of a function.  This method was first described by Lyness and Moler \cite{lyness1967numerical,lyness1968differentiation} and has more recently been rediscovered for use in engineering analysis.  The basic idea is that a model parameter can be made complex and expanded in a Taylor series about a small perturbation along the imaginary axis by some arbitrary value $h$ as follows
%
\begin{align} 
    f ( x + i h ) =& f(x) + \frac{\partial f(x)}{\partial x} \frac{i h}{1!}  \notag \\ &+ \frac{\partial^2 f(x)}{\partial x^2} \frac{(i h)^2}{2!} + \frac{\partial^3 f(x)}{\partial x^3} \frac{(i h)^3}{3!} + \cdots.
\label{eqn:complexTaylor} 
\end{align}
%
Taking the imaginary part of both sides of equation (\ref{eqn:complexTaylor}) and solving for the first derivative and ignoring terms of $\mathcal{O}(h^2)$ yields an estimate
%
\begin{equation} \frac{\partial f( x )}{\partial x} = \frac{\mbox{Im} \left( f (x + i h) \right)}{h} +\mathcal{O}\left( h^2 \right).  
\label{eqn:complexFirstDeriv} 
\end{equation}
%
Thus, an estimate of the first derivative of a function can be made by only utilizing one functional evaluation of a perturbed model parameter along the imaginary axis.  The step-size $h$ is arbitrary and can be made as small as practical (even to machine precision without the dangers of roundoff error as in finite differences) to yield an accurate estimate of this derivative.  The only disadvantage of this technique is the necessity of requiring the functions to accept complex numbers as arguments. Appendix~\ref{sec:appendixA} includes a simple example that illustrates the use of the complex-step method for the derivative calculation of a function. 
%
\todo[]{Re: II.6}For completeness, the forward and central difference formulas corresponding to equation~(\ref{eqn:complexFirstDeriv}) are \cite[Chap. 4.1.3]{chapra2010}
%
\begin{equation} 
\frac{\partial f( x )}{\partial x} = \frac{f (x + h) - f (x)}{h} +\mathcal{O}\left( h \right), 
\label{eqn:forwardFirstDeriv} 
\end{equation}
%
\begin{equation} 
\frac{\partial f( x )}{\partial x} = \frac{f (x + h) - f (x - h)}{2h} +\mathcal{O}\left( h^2 \right). 
\label{eqn:centeredFirstDeriv} 
\end{equation} 
%
The order of the truncation error in the forward difference formula (equation~(\ref{eqn:forwardFirstDeriv})) indicates that for a given $h$ this method is less accurate than either central difference or complex-step which have errors of $\mathcal{O}(h^2)$.

\todo[]{Re: I.4}As mentioned earlier, the CS method was originally introduced in a paper by Lyness and Moler \cite{lyness1967numerical}.  A follow-on contribution by Lyness demonstrated a ``truncated'' approximate version of the original complex-step (CS) method to make it suitable for use on digital computers \cite{lyness1968differentiation}.  After these original papers, little can be found in the literature regarding the method until Squire and Trapp showed through numerical experiments that CS offers superior accuracy to finite-difference (FD) and central-difference (CD) \cite{squire1998using} in computing numerical derivatives. Papers by Newman et al.\ \cite{newman1998} and Anderson et al.\ \cite{anderson2001sensitivity} discuss computing sensitivities in a computational fluid dynamics (CFD) analysis using CS and compares the results to sensitivities produced with central difference derivatives. The work is also notable for identifying automatic-differentiation as a competing method to CS and suggesting that complex-step could be used to compute tangent-stiffnesss of residuals for use in Newton-Krylov schemes for solving nonlinear systems. Perez-Foguet et al.\ \cite{perez2000numerical, perez2012numerical} used the CS method to compute algorithmically consistent elastoplastic moduli in nonlinear solid mechanics problems.  A paper by Joaquim et al.\ \cite{martins2000automated} investigates using CS for computing aerodynamic sensitivities. Additionally, higher-order accuracy versions of CS are discussed, as is the task of implementation in most widely used general purpose programming languages. Discussed is the relative ease of implementing a CS scheme versus an automatic-differentiation based scheme for computing sensitivities in an established code.  

Martins et al.\ \cite{martins2003complex} demonstrates that the CS method can be performed by using complex datatypes for the variables of functions comprising an analysis code. The accuracy of this approach, with respect to the derivatives themselves, is compared to automatic-differentiation in a serial analysis code. Lai and Crassidis \cite{lai2008extensions} discuss first- and second-order CS methods for computing derivatives and choosing optimal step sizes.  Al-Mohy and Higham \cite{al2010complex} investigate the use of CS in computing Fr\'{e}chet derivatives of matrix functions. Jin et al.\ \cite{jin2010improved} discuss using CS to compute sensitivities in a solid mechanics finite element method (FEM) analysis.  Voorhees et al.\ \cite{voorhees2011complex} discusses the background of the CS method and frames it as a subset of Fourier Differentiation. CS and Fourier Differentiation are then applied to computing sensitivities within a solid mechanics finite element analysis. They implemented the complex-values as additional degrees of freedom in the finite element stiffness matrices using a real-valued matrix representation of the complex-numbers where required. In a follow-on work,  Millwater et al.\  \cite{millwater2013application} extended this concept, calling it ZFEM, and implemented the method into user defined element types for use with the popular ABAQUS \cite{systemes2012abaqus} finite element analysis (FEA) software package.  Nishida et al.\ \cite{nishida2013} used the complex-step method to compute tangent-stiffness matrices for implementation in a Levenberg-Marquardt optimization routine for design-of-experiments purposes.  Finally, Abreu et al.\ \cite{abreu2013generalization} discusses computing derivatives with a version of the CS method which features the combined use of real and imaginary valued step components, as in Fourier differentiation.  They suggests that the modified CS method can achieve extended approximation accuracy up to fourth order.  This recent literature has introduced the application of CS to single-dimension and multidimensional derivatives, sensitivity analysis, nonlinear solution schemes, optimization, the extension of CS for higher accuracy, and implementation strategies for CS in new and existing codes.  The current paper presents a comparison of the complex-step method to the other methods for numerical differentiation in as close to a one-to-one implementation as possible for direct comparison purposes in a large-scale engineering analysis code. To the author's knowledge it is the first application of the complex-step method to solve peridynamics mechanics problems, and the first implementation and comparison of the method in large-scale distributed memory parallel setting. 

\subsubsection{Automatic differentiation} 
\label{ADsubsection}

Automatic differentiation (AD) is a computerized method for computing exact derivatives based on the chain-rule from calculus. AD takes advantage of the fact that any mathematical function executed on a computer, no matter how complicated, is a ``composition of simple operations'' (addition, multiplication, exponentiation and the like) each having known analytical derivatives \cite{ref-sacado-presentation}. For reference, the AD implementation used in this study is part of the \emph{Sacado} package from the \emph{Trilinos} agile-component libraries developed at Sandia National Laboratories \cite{ref-Sacado}.

An AD system evaluates a composition of functions in the expected manner: it works by first evaluating the innermost function of the composition, then presenting that function's output as input to the next level function until all levels are complete, observing commonly expected order of operations. However, an AD system does additional work during a function evaluation in that as each nested function is evaluated, the function's partial derivative with respect to the designated variables of the given input is also calculated.  This is possible because the elementary math functions are hard-coded into the AD source-code along with their analytical derivatives, and linked by special instructions, so that when the elementary math functions are called upon for computation, their partial derivatives may be computed and stored in a sequence. The AD system then multiplies the final sequence of partial derivatives together to produce the exact equivalent to taking a partial derivative of the corresponding composition function with respect to a designated variable at a particular value. 
%
It is obvious, but bears mentioning, that the AD system could simply store one value for partial derivatives, modifying it as appropriate for every function evaluation rather than keeping a sequence. This is significant because modifying a single value rather than keeping a list of values corresponding to every level of a composition function, which itself may not be the sole one needing to be evaluated, represents a savings in memory usage, which at large scales is an active concern. In the literature, the AD scheme described here is called \emph{forward automatic differentiation}. Only forward AD will be covered here since it is the implementation used in \emph{Sacado} and therefore in this study; however, the reader is referred to the introduction section of \cite{ref-AD-methods} and its citations for further information on AD, and
particularly \cite{ref-on-AD} which is foundational.

Appendix~\ref{sec:appendixB} includes an example that walks the reader through the process a computer uses to compute derivatives via AD.  Some things to note about AD are that no approximation of derivatives is being made because the analytical forms of the partial derivatives of the elementary math functions are defined alongside them. The accuracy of AD is then limited by the precision of the AD system's definition of the elementary math functions and their partial derivatives.
 
\subsection{Tangent-stiffness} 
\label{subsec:TS}

In solid and structural computational mechanics the tangent-stiffness matrix is a linearization operator that describes the stiffness of a system in response to small displacements imposed upon the current configuration of the system.  Mathematically speaking, the tangent-stiffness represents the gradient of a high-dimensional energy surface that ``points'' in the minimum direction. While the terminology of solid mechanics is used here, it's important to note that these operators appear in other physical settings and are known by other names, e.g. a \emph{transmissibility matrix} in the context of a Poisson problem or, more generally, a \emph{Jacobian matrix} in mathematical optimization \cite{morton2005numerical}.

While, for simplicity, the examples shown in the following refer only to linear problems, tangent-stiffness matrices generally arise in the context of nonlinear analysis where Newton's method or quasi-Newton methods are used in the minimization of a given residual function.  In this context, the tangent-stiffness matrix can be thought of as the linearization of the system about a particular configuration.  It has been shown \cite{hughes1978consistent,hughes1978unconditionally} that in order to preserve the quadratic convergence properties of the Newton's methods, it is necessary that this linearization is carried out in a manner that is \emph{consistent} with the algorithmic constraint equations used when computing the internal forces that arise due to deformations.  An example of these constraint equations would be the Kuhn-Tucker conditions \cite{simo1998} that are used in integration of a flow rule for plasticity modeling.  If the continuum tangent-modulii are used in place of the \emph{consistent} or \emph{algorithmic tangent moduli} in the solution of a nonlinear solid mechanics problem, convergence may still be achieved, but not in the quadratic manner that makes Newton's method attractive for this class of problems.  In the setting of nonlinear solid mechanics, there is a very small class of material model algorithms in which consistent tangent-stiffness operators can be derived analytically.  Perfect plasticity and plasticity with isotropic hardening used in combination with a general nearest-point projection or \emph{radial return algorithm} are a few examples.  Therefore, when general models (and certainly more complex ones) are implemented into a general purpose computational mechanics code, the tangent-stiffness operators are typically defined via a numerical approximation.  

A general mathematical formula for a tangent-stiffness matrix, $K_{ij}$, can be expressed in indicial notation as 
%
\begin{equation} K_{ij} = \left. \frac{\partial F_i}{\partial
X_j}\right|_{\vec{X}_0}, \end{equation}
%
where $F_i$ is the $i^{\mbox{th}}$ component of the vector valued function, $X_j$ is the $j^{\mbox{th}}$ component of the vector argument of $F$, and a particular value of the vector, $\vec{X}_0$ is chosen as the linearization point. One then evaluates the expression for each combination $(i, j)$ corresponding to a (row, column) location in the tangent-stiffness matrix. The elements of a tangent-stiffness matrix can be estimated using any of the complex-step, AD, or common finite difference techniques for functions $F:R^1 \rightarrow R^1$, since taking partial derivatives entails holding all but a single independent vector component of the argument of the function constant, and each element of the function can be evaluated independently of the others. \\ 

\subsubsection{Differencing formulas for tangent-stiffness matrix evaluation}

The \emph{forward difference} (FD) formula for calculating a tangent-stiffness matrix in the setting of a solid structural mechanics problem is 
%
\begin{equation} 
  K_{ij} \approx \frac{F_i^{int}(\vec{u} + h \hat{e}_j) - F_i^{int}(\vec{u})}{h},
\end{equation}
%
where $K_{ij}$ is the indicial notation representation of an element of the tangent-stiffness matrix at row $i$, column $j$. The first of the two terms in the numerator is the internal force, $F_i^{int}$ evaluated in the current deformed configuration, as a function of the displacement, $\vec{u}$,  plus a small perturbation $h$ in the direction $\hat{e}_j$ where $\hat{e}_j$ represents a unit-vector corresponding to the $j^{\mbox{th}}$ degree-of-freedom. The second of the two terms is $F_i^{int}$ evaluated in the current configuration. The denominator is the
magnitude of perturbation called the \emph{probe-distance}.

The \emph{central difference} (CD) formula for calculating a tangent-stiffness matrix is
%
\begin{equation} 
  K_{ij} \approx \frac{F_i^{int}(\vec{u} + h \hat{e}_j) - F_i^{int}(\vec{u} - h \hat{e}_j)}{2 h}.
\end{equation}
%
Together, the FD and CD methods, are sometimes termed \emph{finite difference probing} techniques for tangent-stiffness matrix calculation in the literature \cite{ref-Adaggio}. From the well-known 1-dimensional formulas for FD and CD, it is understood that the accuracy of these expressions is dependent theoretically upon selecting a small step-size $h$, however experiments shown in \cite{squire1998using} demonstrate that too small an $h$ can lead to inaccuracy from ``subtractive-cancellation''. Therefore it is fair to say that a general method for selecting $h$ in practice is unobtainable.

The complex-step (CS) formula for calculating a tangent-stiffness matrix is
%
\begin{equation} K_{ij} \approx \frac{\mbox{Im}(F_i^{int}(\vec{u} + i h
\hat{e}_j))}{h}, \end{equation}
%
where the internal force, $F^{int}$ is now treated as function of complex vectors,  $\vec{u}$. The small perturbation, $h$, is now carried out on the imaginary axis and only the real coefficient to the imaginary part of the vector returned by $F^{int}$ is kept, hence the $\mbox{Im}(\cdot)$ operation.  As highlighted in Section~\ref{sec:CSmethod}, notice that there is no subtraction operation taking place; therefore $h$ can be made very small to yield highly accurate derivatives. It should be mentioned that using complex-step to compute tangent-stiffness matrices was done in \cite{perez2000numerical,perez2012numerical}, but those references did not
investigate the performance of complex-step in a massively parallel setting nor make comparisons to AD. 

When using automatic differentiation to calculate a tangent-stiffness matrix, one only needs to follow the definition of the tangent-stiffness matrix and issue the correct commands to the AD system. The formula is the same as the continuum formula
%
\begin{equation} K_{ij} \approx \frac{\partial F_i^{int}(\vec{u})}{\partial u_j},
\end{equation}
%
but, with the caveat that $F^{int}$ is evaluated in a way that is algorithmically consistent, and therefore should yield quadratic convergence with Newton's method.

\section{\todo[]{Re: I.5}Simple example of CS, AD and FD for solution of nonlinear problems.}
\label{sec:Validation}

In order to illustrate a comparison of the numerical differentiation techniques in a simpler form than it appears in the large-scale analysis code covered in the following sections, a simple example borrowed from \cite{rezaiee2010dynamic} was implemented with each of the methods, CS, AD, and FD. The reason for including FD was to provide an example of implementing a familiar differentiation technique alongside the more unusual CS and AD examples. CD is omitted for brevity in the code development for the example. All the source code to reproduce the results described here are available in the associated paper repository. This source describes the implementation and interface to the \emph{Trilinos Secado library} for automatic differentiation.

The example problem is a single degree-of-freedom nonlinear mechanical system involving a connected truss member and spring as illustrated in Fig. \ref{fig:TrussSchematic}. 
%
\begin{figure}[tbp] 
\centering %%\includegraphics[width=0.7\textwidth]{./figs/truss.png}
\begin{tikzpicture}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{decorations.pathmorphing}
\usetikzlibrary{calc}
\usetikzlibrary{patterns}
\tikzstyle{wall}=[fill,pattern=north east lines,draw=none,minimum width=0.2cm,minimum height=5.1cm];
\tikzstyle{ground}=[fill,pattern=north east lines,draw=none,minimum width=6.1cm,minimum height=.2cm];
\tikzstyle{displacement_reference}=[fill,pattern=north east lines,draw=none,minimum width=5.cm, minimum height=.6cm];
\coordinate (bar_1) at (0,0);
\coordinate (bar_1_support) at (0,-.5);
\coordinate (bar_2) at (5,3);
\coordinate (bar_2_pin) at (5.4,3.0);
\coordinate (spring_1_support) at (5,-.5);
\coordinate (spring_1_pin) at (5,0);
\coordinate (rhm) at (5.4,1.5);
\coordinate (rhml) at (4.5,1.5);
\coordinate (mf) at (2.5, -0.8);
\coordinate (lm) at (1.0, 0.0);
\coordinate (mm) at (1.2, 0.0);
\coordinate (tm) at (2.5, 2.0);
\coordinate (rht) at (5.0, 4.0);
\coordinate (rhtt) at (5.0, 4.2);
\coordinate (dispref) at (6.5, 3.06);
\coordinate (bmdisref) at (6.5, 3.0);
\coordinate (bldisref) at (6.0,3.0);
\coordinate (brdisref) at (7.0,3.0);
\coordinate (bbdisref) at (6.5,2.5);
\coordinate (bbrdisref) at (6.8,2.6);
\node (displacement_reference) at (dispref) [rectangle, draw, style=displacement_reference, scale=.2]{};
\node (wall_slider) at (bar_2_pin) [circular sector, draw, rotate=180] {};
\node (ground_triangle_1) at (bar_1_support) [isosceles triangle, draw, rotate=90] {};
\node (ground_triangle_2) at (spring_1_support) [isosceles triangle, draw, rotate=90] {};
\node (wall) at ($ (rhm) !.3cm!(10, 3) $)  [rectangle, draw, style=wall] {};
\node (ground) at (mf)  [rectangle, draw, style=ground] {};
\node (spring_label) at (rhml) [] {$k_s$};
\node (load_label) at (rhtt) [] {$P$};
\node (bar_length_label) at (tm) [] {$L_0$};
\node (angle_label) at ($ (mm)!.4cm!(bar_2) $) [] {$\phi$};
\node (displacement_label) at (bbrdisref) [] {$D$};
\path (bar_1) -- (bar_2) [draw, very thick];
\path (bar_1) circle (.10) [fill=black];
\path (bar_2) circle (.10) [fill=black];
\path (spring_1_pin) circle (.10) [fill=black];
\path [decorate, decoration={coil, segment length = 4, amplitude=4}] (bar_2) -- (spring_1_pin) [draw, thin, stroke=black];
\path (rht) -- ($ (bar_2)!.2cm!(rht) $) [draw, -latex, ultra thick, color=black];
\path (lm) arc(0:16:2) (lm) [draw];
\path (bar_1) -- (mm) [draw, -latex];
\path (bldisref) -- (brdisref) [draw];
\path (bmdisref) -- (bbdisref) [draw, -latex];
--cycle
\end{tikzpicture}
\caption{Beam and spring simulated in the example program.} 
\label{fig:TrussSchematic}
\end{figure}
%
For this system, the equilibrium reaction force associated with a displacement $D$ of the spring is 
%
\begin{equation} 
    \label{eqn:TrussForce}
    f(D) = \frac 1 2 AE(\cos^{2}\phi)\left(\frac{D}{L_{0}}\right)^{2}\left[\frac{D}{L_{0}}\cos^{2}\phi - 3\sin\phi\right] + k_{s}D + \left(AE\frac{D}{L_{0}}\right)\sin^{2}\phi,
\end{equation} 
%
and the analytical tangent-stiffness is 
%
\begin{equation} 
    \label{eqn:TrussStiffness}
    S_{T}(D) = \frac 3 2 AE(\cos^{2}\phi)\left[\frac{D}{L_{0}}\cos^{2}\phi - 2sin\phi\right]\left(\frac{D}{L_{0}^{2}}\right) + k_{s} + \frac{AE\sin^{2}\phi}{L_{0}}.
\end{equation}
%
where $\phi$ is a constant truss angle, $AE$ is the axial rigidity of the truss, $L_{0}$ is original truss member length, $k_{s}$ is the Hooke's constant for the spring attached to the truss member, and $P$ is the applied load. $\phi$ is considered constant for small displacement for initial truss angles close to zero. For a demonstration and verification of each of the methods as well as a convergence rate study, this truss and spring model was analyzed with each of the methods and compared with the analytic solution. As in the reference \cite{rezaiee2010dynamic}, equilibrium solutions for 12 incremental load steps of $\Delta P=\SI{4.448}{\newton}$ were found. Solver parameters common to all tests were a maximum limit of 100 iterations for convergence, and a convergence tolerance for the change in displacement over an iteration $\Delta D \le \SI{1e-9}{\meter}$.  Data collected included number of iterations to converge summed over all load-steps and final displacement $D$, after the final load-step. Additionally finite difference and complex-step step sizes were varied in order to measure the impact on each of the approximate tangent-stiffness calculation methods. Naturally, varying the step size does not affect AD or the analytically derived tangent-stiffness since those are not functions of the step size. The accuracy results for verifying the tangent-stiffness calculation methods are summarized in Table~\ref{tab:ConvergenceStudy}. 
%
\begin{table}[tbp]    
  \centering
  \caption{Displacement, D, after load-step 12} \label{tab:Verification}   
        \begin{tabular}{c c c c c}
         \toprule
         Step exponent & Analytical ($\si{\centi\meter}$) & AD ($\si{\centi\meter}$) & CS ($\si{\centi\meter}$) & FD ($\si{\centi\meter}$)\\ 
        \midrule
        0 & 5.07996 & " & " & "\\
        -4 & 5.07996 & " & " & "\\
        -8 & 5.07996 & " & " & "\\
        -12 & 5.07996 & " & " & " \\
        -16 & 5.07996 & " & " & failed \\
        -20 & 5.07996 & " & " & failed \\
        \bottomrule
    \end{tabular}
\end{table}
%
The label ``Step Exponent'' refers to the value of $n$ in $h=10^{n}$ where $h$ is the finite or complex-step step size, and the symbol (") indicates that there is no change in the value from the value directly to its left.  Table.~\ref{tab:Verification} shows that each of the methods solved the problem as accurately as one another within the precision reported. The exception to this is that the finite difference method failed due to an arithmetic error in association with extremely small step size, suggesting subtractive cancellation. However, since there is no accuracy reason for choosing an extremely small step size, each of the methods can be used to produce acceptable solutions to the nonlinear truss system. Table~\ref{tab:ConvergenceStudy} shows the total number of iterations taken over all load-steps for each of the methods. 
%
\begin{table}[tbp]    
  \centering
  \caption{Summed iterations over all load steps.} \label{tab:ConvergenceStudy}   
        \begin{tabular}{c c c c c}
         \toprule
         Step exponent & Actual ($\si{\centi\meter}$) & AD ($\si{\centi\meter}$) & CS ($\si{\centi\meter}$) & FD ($\si{\centi\meter}$)\\ 
        \midrule
        0 & 63 & " & 213 & 417\\
        -4 & 63 & " & " & "\\
        -8 & 63& " & " & "\\
        -12 &63& " & " &  65\\
        -16 &63& " & " & fail\\
        -20 &63& " & " & fail\\
        \bottomrule
    \end{tabular}
\end{table}
%
Table~\ref{tab:ConvergenceStudy} indicates that when using AD or the `actual' analytically derived tangent-stiffness, a minimum number of iterations is achieved corresponding to a maximum convergence rate. CS and FD are similarly affected by a grossly large step-size, while subtractive cancellation invalidates the results for FD for very small step-sizes. Inaccuracy for large step-sizes is as predicted by the FD and CS formulas, which require a small step-size to allow the neglect of higher order terms. The results of this test show that step-size selection is not unimportant when using FD, but when properly configured any of the methods may return the same maximum convergence rate for this problem.  Conclusions regarding the methods that the accuracy of the tangent-stiffness should not be confused with the accuracy of the equilibrium solution, since each of the methods discussed here may be equally capable given enough iterations. Numerical measurements of accuracy besides number of iterations taken are not possible here because there is not basis for comparison, as the methods are solving the problem separately and producing different local Newton iterates. Additionally, while the inaccuracy of the tangent-stiffness may strongly affect convergence rates, this relationship was not precisely quantified here.

\section{Description of analysis tools and approach.} 
%
\subsection{Peridigm and peridynamics.}
\label{subsec:PaP}
%
Comparisons of the different tangent-stiffness matrix calculation techniques were carried out through code-development and instrumentation of the computational peridynamics code \emph{Peridigm} \cite{peridigm}.  \emph{Peridigm} is distributed by Sandia National Laboratories as its primary open-source computational peridynamics code. It is a massively parallel simulation code for implicit and explicit multi-physics simulations primarily used for solid mechanics and material failure. \emph{Peridigm} is a C++ code utilizing agile software components from Sandia's \emph{Trilinos} project \cite{trilinos}. It's important to note that there is no specialization of the tangent-stiffness matrix calculation methods described previously to this particular code or more generally to a computational peridynamics approach and the analysis carried out in this study should provide insight into the speed and accuracy of these methods when implemented into other computational mechanics analysis tools as well.  \emph{Peridigm} was chosen primarily because the FD, CD, and AD methods for tangent-stiffness matrix calculation where already implemented in the code, leaving only the CS method for development. Also, because peridynamics is a nonlocal theory, the tangent-stiffness matrices have a much higher bandwidth (i.e. less sparsity) than traditional finite element tangent-stiffness matrix computations and the time required to construct the tangent-stiffness matrix is the majority of the total computation time in any quasi-static or implicit dynamics analysis; therefore, the \emph{Peridigm} development team has an interest in profiling the performance of the tangent-stiffness matrix computation methods.

\todo[]{Re: I.2} Peridynamics \cite{silling2000ret,silling:psa,silling2010peridynamic} is a nonlocal reformulation of the partial-differential equations that provide the statement of momentum balance in classical continuum mechanics. Its primary goal is to avoid the use of spatial derivatives in the balance equations or constitutive laws such that discontinuous displacements, i.e. cracks, are mathematically consistent with the governing equations. It has demonstrated great promise in modeling problems with pervasive failure \cite{littlewood2010}, interesting phenomenon such as dynamic crack branching \cite{ha2010sod}, and is being extended to multiphyics phenomena \cite{bobaru2011peridynamic,katiyar2013} and the problem of multi-scale coupling to molecular simulations \cite{seleson2009peridynamics,seleson2014atom,rahman2013b}.  The material model used in this study was a linear peridynamic solid (LPS) which replicates classical elasticity in the \emph{local limit}, i.e.\ when the non-local length scale vanishes. While \emph{linear} may appear in the name of the material model, it is geometrically nonlinear under large deformations and requires a nonlinear solver to converge to equilibrium conditions in these situations.  The peridynamic equation of motion is 
%
\begin{equation} 
    \rho(\mathbf{x})\mathbf{\ddot{u}(x)} = \int_{\mathcal{H_{\delta(\mathbf{x})}}} \mathbf{f(x,x'}) {\rm d}V_{\mathbf{x'}} + \mathbf{b(x)},
    \label{eqn:PDMotion}
\end{equation} 
%
where, $\rho$ is the mass density, $\mathbf{\ddot{u}}$ is the second time derivative of displacement field (i.e.\ acceleration), and $\mathcal{H}_{\delta(\mathbf{x})}$ is a neighborhood containing remote material points, $\mathbf{x'}$ that are closer to $\mathbf{x}$ than the characteristic length $\delta$, called the peridynamic \emph{horizon}. The integrand $\mathbf{f}$ contains the constitutive response of the material (see \cite{silling:psa} for the exact form of $\mathbf{f}$ in the LPS model).  Physically, $\delta$ is a material length-scale and computationally it, along with the discretization, will set the bandwidth of the tangent-stiffness matrix. $\mathbf{b}$ is a body force density.  An explicit dependence on time $t$ has been supressed from the functional arguments, but is implied.  The reader is refered to the seminal work on peridynamics \cite{silling:psa} for detailed derivations of the momentum equation and example consitutive models.

\subsubsection{\todo[]{Re: I.2}The tangent stiffness matrix in a quasi-static peridynamic simulation}
The tangent-stiffness matrix is the Fr\'{e}chet derivative of the residual with respect to the deformed positions of the discrete computational nodes.  For the purposes of this work, the residual represents the unbalanced forces in the peridynamic equilibrium equation.  For the finite difference type methods in \emph{Peridigm}, a perturbation method is employed by probing the deformed configuration one degree-of-freedom at a time and so allowing the derivative of the residual for all nodes to be computed.  The resulting spatial rate of change in the residual for each of these independent probes becomes one column of the tangent-stiffness matrix.

\subsubsection{Implementing the complex-step method in Peridigm} 
\label{subsubsec:Impl}
%
The CS method was implemented in \emph{Peridigm} in a manner that follows closely the implementation of the FD technique.  However, because of \emph{Peridigm}'s reliance on \emph{Trilinos} and specifically the \emph{Epetra} package, special steps had to be followed to allow the use of complex number data types.  This is because \emph{Epetra} vectors, which can be thought of as distributed memory parallel cousins to standard C++ Standard Template Library (STL) vectors, are hard coded to be of type {\tt double} and can not be simply declared as having a complex data type. \todo[]{Re: I.3} \emph{Epetra} vectors and STL vectors are what are known as \emph{container classes}, whose purpose in object-oriented computer programming is to automate the creation, accessing, modification,  and destruction of the underlying contained data.  \emph{Templated} container classes allow for user choice in what properties that underlying data exhibits.  For example, in this work, complex arithmetic cannot be performed on the data contained within an \emph{Epetra} vector because that data, of type {\tt double},  does not have the property of following the rules of complex arithmetic. The reason the non-templated \emph{Epetra} vector could not be dispensed with for the purposes of this work is because the \emph{Epetra} vectors are tightly integrated with \emph{Peridigm}'s parallel communication strategy and linear algebra code. Because of restrictions imposed by the container class being used, this involved changing the data type of intermediate variables which were locally scoped (i.e. only allocated to memory within the specific CS method and later destroyed), and dynamically casting persistent memory variables to complex data types where necessary. No attempt was made to ``tune'' the CS code in any way for performance. The algorithm  was implemented in such a way that it was in one-to-one correspondence with the FD and CD methods; however, it must be noted that the overhead associated with the dynamic casts in the CS method likely hurts its overall performance and this issue could be alleviated by using the next generation of templated \emph{TPetra} vectors available in \emph{Trilinos} that are capable of taking an explicit instantiation of any data type including {\tt complex}. The version of \emph{Peridigm} modified to implement CS can be found in the paper repository.  

\emph{Peridigm} was primarily written by computational scientists and engineers; therefore, many advanced techniques in C++ such as multiple virtual inheritance, pointer arithmetic, and distributed data structures are employed. These concepts are not vital to understanding the CS method itself, but may be necessary to understand its implementation in \emph{Peridigm}. To see CS and other methods in a more narrow context, it is advised that readers also take a look at the simple examples, such as that described in Section~\ref{sec:Validation},  provided on the corresponding author's website.

\subsection{Comparative study} 
\label{tcs}

The methods were compared by running two sets of test problems where each method would solve a problem concurrently. In this context, concurrently means successively, drawing from the same independent variables, yet within the same computational process; this definition is characterized in further detail in Section~\ref{JGAM}. Each test problem simulated a $\SI{4}{\meter}$ by $\SI{0.5}{\meter}$ by $\SI{0.5}{\meter}$ meter block of a material undergoing tension along the axis parallel to the long dimension of the block. The test problems were set up in \emph{Peridigm} as quasi-static equilibrium problems, where the displacement boundary conditions used to apply tension were applied gradually in ``load steps'', and an equilibrium solution for each load step was achieved before applying the next load step. Applying the load in gradual steps is a widely used technique to ensure the Newton's method is always initialized near the actual solution and therefore likely to converge.  The fictional material represented in the model had values of $\SI{1.515e4}{\mega\pascal}$ for bulk modulus, and $\SI{7.813e4}{\mega\pascal}$ for shear modulus. The peridynamic horizon  used in the test problems was always set to three times the nominal discretization size (i.e.  node spacing in the particle discretization scheme). Pictured in Figure.~\ref{fig:pd_discrete} is an illustration of the peridynamic discretizations used in this study. The coloration in the images indicate material regions that were defined to assist in post-processing and application of boundary conditions. 
%
\begin{figure}[tbp] 
    \centering 
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{./figs/mesh2000_replacement.png}
        \caption{2000 node mesh.}
        \label{fig:SCMesh}
    \end{subfigure}
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{./figs/mesh1000000_replacement.png}
        \caption{1000000 node mesh.}
        \label{fig:MCMesh}
    \end{subfigure}
    \caption{Peridynamic discretizations used in study.}
    \label{fig:pd_discrete}
\end{figure}

A series of nine single compute core (i.e.\ serial) test problems were run with the discretization size being refined with every test in the series. The aim of increasing the refinement was to examine differences between the methods in terms of accuracy and speed. This series would comprise the single core runs.  \todo[]{Re: II.16} The specific parameters of these tests can be found in \emph{Peridigm} input {\tt xml} files included in the paper repository, however the parameters are reproduced in Table~\ref{tab:TestParams}. These {\tt xml} files allow users with the appropriately modified version of \emph{Peridigm} to reproduce the results found in this paper.
%
\begin{table*}[!tbp]    
  \scriptsize
  \centering
  \caption{Test parameters.} \label{tab:TestParams}   
       \begin{tabular}{c c c c c c c}
         \toprule
				 Num. cores &Peri. nodes & Nozero TS elems. & Horizon ($\delta$) & Load rate & Number Load steps & Abs. tolerance \\ 
         &            & $\times 10^6$    & \si{\centi\meter}& \si{\centi\meter\per\second} &   &     \\
        \midrule
        1 & 1000 & $1.99$             & $30$   & $2$ & $2$ & $\num{10E-8}$ \\
        1 & 2000 & $4.25$             & $24$  & $2$ & $2$ & $\num{10E-8}$ \\
        1 & 3000 & $7.79$             & $21$  & $2$ & $2$ & $\num{10E-8}$ \\
        1 & 4000 & $15.4$             & $19$  & $2$ & $2$ & $\num{10E-8}$ \\
        1 & 6000 & $20.0$             & $16$  & $2$ & $2$ & $\num{10E-8}$ \\
        1 & 8000 & $31.4$             & $15$  & $2$ & $2$ & $\num{10E-8}$ \\
        1 & 12000 & $40.1$            & $13.1$ & $2$ & $2$ & $\num{10E-8}$ \\
        1 & 16000 & $83.9$            & $12$  & $2$ & $2$ & $\num{10E-8}$ \\
        1 & 32000 & $165.0$           & $9.5$ & $2$ & $2$ & $\num{10E-8}$ \\
        32 & 1000000 & $1670$          & $3$  & $2$ & $2$ & $\num{10E-8}$ \\
        64 & 1000000 & $1670$          & $3$  & $2$ & $2$ & $\num{10E-8}$ \\
        96 & 1000000 & $1670$          & $3$  & $2$ & $2$ & $\num{10E-8}$ \\
        128 & 1000000 & $1670$          & $3$  & $2$ & $2$ & $\num{10E-8}$ \\
        \bottomrule
    \end{tabular}
\end{table*}

Another series of four test problems were simulated, however this time the number of compute cores used to solve the problem was increased from test to test while the discretization level was held constant at 1 million computational nodes (3 million degrees of freedom). This allowed for sufficient computational nodes per core even at the highest level of parallelization such that message passing computation did not overwhelm the simulation.  The specific parameters of these tests can be found in \emph{Peridigm} input {\tt xml} files also included in the paper repository, but a summary of the most relevant ones appears in Table~\ref{tab:TestParams}. An image of the 1 million node discretization is shown in Figure~\ref{fig:MCMesh}. The nodes in the image, although actually discrete points in space are dense enough to appear as a continuous body at a distance.

\subsubsection{Quantities-of-interest} 
\label{JGAM} 

As stated in Section~\ref{sec:intro}, a goal of this study was to compare CS, FD, and CD on the basis of accuracy. AD was omitted from the comparison because it served as the standard of accuracy for the other methods in the absence of appropriate analytical forms for the tangent-stiffness matrix associated with the system solved in this study. The assumption that AD is accurate enough to serve as a standard is supported by AD's implementation as a computerized chain rule as explained in Section~\ref{ADsubsection}.

It would be a poor comparative study to compare tangent-stiffness matrices from different problems, load steps that start with different current configurations, or from different iterations; because of this, it was necessary to solve one load step and conclude each Newton iteration within that load step by updating the displacement iterate with only the results of the AD method and to subsequently feed all four methods the same updated displacement in the following iteration. This decision precluded a comparison of Newton iteration convergence rate, since if the methods were allowed to solve a problem at their individual pace, differences in accuracy would produce differences in guess updates and therefore the number and nature of Newton iterations performed (e.g. lower accuracy predictions of the algorithmically correct tangent-stiffness matrix could cause a loss of quadratic convergence).  Different guesses from iterations started with different previous guesses could not be data for a valid comparison of tangent-stiffness matrix accuracy between methods.  Additionally, having each of the methods physically operate in the same process, serial or one of associated parallel, allowed the comparison of tangent-stiffness matrix calculation time as cached from random access memory rather than from the hard-disk.  Caching from RAM gives the benefit of vastly greater speed and simpler programming compared to some other solution speculatively involving dynamic file management on files which for one of the components of this study would be on the order of a terabyte in size. However, the price of running the methods together in the same process and avoiding the use of the hard-disk was that at least two tangent-stiffness matrices had to be stored in RAM during the simulation, which meant that special high-memory compute nodes were needed for the 1 million peridynamic node test series.

The other goal was to compare the four methods, including AD, on the basis of speed. Speed is defined as the total compute-time of one iteration of a Newton step which includes the tangent-stiffness matrix calculation and parallel assembly.  The speed of iteration was measured because it could be so done at the same time as accuracy was being measured given a single tangent-stiffness calculation.  It was assumed that the order that each method was evaluated in parallel was unimportant, that evaluating each method successively within each solver iteration did not affect their performance individually and that speed of computation did not change with time. These assumptions allowed the test program to run the same problem with each of the methods at effectively the same time and generate an equal volume of data from each method. It was also assumed that the tangent-stiffness matrix calculation time for each of the methods did not vary based on the current configuration, as it changes slightly from iteration to iteration as the Newton solution iterate is updated.  This assumption allowed calculation time measurements to be averaged over all iterations within a single load step. The purpose of averaging calculation time measurements was to informally address the extraneous variables of parallel evaluation order and computer system load due to system processes not associated with this study. While not part of this study, the example programs available in the paper repository allow the reader to make a comparison of the methods for themselves on the basis of accuracy and convergence rate for two example nonlinear systems. In these examples, one will notice that the finite difference methods will lose quadratic convergence when the step size selected is too large such that accuracy relative to AD is decreased as shown in Table~\ref{tab:ConvergenceStudy}.

The goals of collecting accuracy and speed measurements were achieved by developing and implementing metrics within the simulation program used in this study. The metric used to measure the accuracy of a tangent-stiffness matrix was the Frobenius norm (analogous to an $l^2$-norm except defined on a matrix) of the element-wise difference between the tangent-stiffness matrix produced by the method being evaluated and the tangent-stiffness matrix produced by the AD based method (i.e., the \emph{exact} derivatives to numerical precision), given that both methods were set upon the same problem. The lower the value of this metric, the more accurate the method. The expression for the accuracy metric was
%
\begin{equation} 
    D = \sqrt{\sum_i \sum_j(K^{AD}_{ij} - K^{M}_{ij})^2},
    \label{eqn:accuracy} 
\end{equation}
%
where $D$ is distance, $K^{AD}$ is the tangent-stiffness matrix produced by the AD based method, and $M$ is replaced with either CS, FD, or CD depending on the method being compared.  The metric used to compare the speed of the different methods was the time in seconds required to calculate the tangent-stiffness matrix. A final basis of comparison used in this study called \emph{computational efficiency} is based on specific calculation time per tangent-stiffness matrix element.

\subsubsection{Computers and other software} 
%
Discretization for the test problems run in this study was done using the software package \emph{Cubit} developed at Sandia National Laboratories \cite{ref-Cubit}. \emph{Cubit} generates a finite element mesh that is converted to a ``particle'' discretization internally when running \emph{Peridigm}. The number of computational nodes in \emph{Peridigm} corresponds to the number of finite elements in the discretization, not the number of finite element nodes.  An example journal script can be found in the corresponding paper repository. 

The test problems were run on the \emph{Stampede} HPC cluster computer housed at \emph{TACC} (Texas Advanced Computing Center) at The University of Texas at Austin. The single core test series was run using the {\tt normal} queue compute nodes, while the parallel test series was run using one to four {\tt large memory} nodes each having one terabyte of local RAM. This large amount of memory was required for reasons explained in Section~\ref{JGAM}.

\subsubsection{Data reduction} 
%
The output produced by \emph{Peridigm} including the additional accuracy and speed measurements was redirected from the console to text files by the resource manager, \emph{SLURM} (Simple Linux User Resource Manager).  Directories and filenames were chosen so that data would be easy to sort by individual test run.  \emph{Python} scripts were then used to post-process and plot the data. These scripts averaged accuracy or speed data for a test corresponding to a particular peridynamic node density over all iterations for that run, and then plotted this averaged data  as a function of peridynamic node density as it varied from test to test in the series.  Similarly, for the parallel tests, the data was averaged and plotted in much the same way, but as a function of number of compute cores used to solve the problem. Confidence intervals were not calculated on the measurements. The main reason is that network traffic from other users makes running tests on the HPC cluster a time varying process. These data reduction and plotting scripts can be found in the paper repository.

\section{Results and discussion.}
\label{sec:Res}
%
Table~\ref{tab:results} shows the compute core count, number of nonzero tangent-stiffness (TS) matrix elements, load step average calculation time and load step average accuracy. The accuracy measure is given by equation~(\ref{eqn:accuracy}) and the choice of elastic modulii mentioned in Section~\ref{tcs}.
%
\begin{table*}[!tbp]    
  \scriptsize
  \centering
  \caption{Averaged results, each test.} \label{tab:results}   
       \begin{tabular}{c c c c c c c c c}
         \toprule
         Cores & TS Elements & \multicolumn{4}{c}{Calculation Time ($\si{\second}$)} & \multicolumn{3}{c}{Accuracy Difference ($\si{\mega\pascal}$)} \\ 
         & $\times 10^6$ & CS & CD & FD & AD & CS & CD & FD \\
        \midrule
        1 & $\num{1.99}$  & 3.5 & 2.7 & 1.7 & 1.6 & $\num{1.92E-10}$ & $\num{1.21E-4}$ & .137 \\
        1 & $\num{4.25}$  & 6.2& 4.9& 3.2& 2.9 & $\num{2.28E-10}$ & $\num{9.94E-4}$ & .148 \\
        1 & $\num{7.79}$  & 11.2& 8.9& 5.7& 5.2 & $\num{2.38E-10}$ & $\num{1.59E-4}$ & .145\\
        1 & $\num{15.4}$  & 26.7& 21.0& 13.4& 12.2 & $\num{2.33E-10}$ & $\num{4.61E-4}$ & .12 \\
        1 & $\num{20.0}$  & 28.1& 22.2& 14.4& 13.1 & $\num{2.76E-10}$ & $\num{1.05E-3}$ & .145 \\
        1 & $\num{31.4}$  & 47.6& 37.7& 24.2& 21.9 & $\num{2.64E-10}$ & $\num{1.65E-3}$ & .133 \\
        1 & $\num{40.1}$  & 55.6& 44.1& 28.4& 25.9 & $\num{3.03E-10}$ & $\num{1.92E-3}$ & .148 \\
        1 & $\num{83.9}$  & 138.9& 109.6& 70.2& 64.0 & $\num{3.63E-10}$ & $\num{1.64E-3}$ & .123 \\
        1 & $\num{165.0}$  & 277.3& 218.1& 139.4& 126.5 & $\num{3.26E-10}$ & $\num{2.18E-3}$ & .128 \\
        32 & $\num{1670}$  & 336.1& 277.7& 200.6& 233.1 & $\num{6.21E-10}$ & $\num{1.52E-2}$ & .176 \\
        64 & $\num{1670}$  & 169.9& 140.7& 102.0& 119.7 & $\num{6.20E-10}$ & $\num{1.50E-2}$ & .177 \\
        96 & $\num{1670}$  & 114.7& 95.0& 69.1 & 79.7 & $\num{6.18E-10}$ & $\num{1.50E-2}$ & .177 \\
        128 & $\num{1670}$  & 86.4& 71.8& 52.4 &58.8 & $\num{6.16E-10}$ & $\num{1.47E-2}$ & .177 \\
        \bottomrule
    \end{tabular}
\end{table*}

\subsection{Speed data} 
%
Speed measurements were taken according to the methods mentioned in Section~\ref{tcs}. The averaged results of these speed measurements for the serial tests can be seen in Fig.~\ref{fig:serial_speed}.  Average time in seconds to compute a TS matrix is plotted as a function of the number of nonzero TS elements.  CS was the slowest of the four methods, taking the longest average time per iteration, followed by CD, FD and AD in that order. The calculation time has a power-law relationship with problem size, with a power-law index of roughly $\num{1.0E-6}$ for each of the methods. This bodes well for the scalability of each of the methods by themselves in \emph{Peridigm}. If we assume that every individual operation takes the same amount of time on average, such that compute time is a measure of number of operations, then this suggests that the methods as implemented in \emph{Peridigm} have the same complexity class when considering computing a TS matrix alone, i.e.\ computation time is $\mathcal{O}(10^{(10^{-6} \times \log_{10}n)})$. The definition of complexity class used here is the informal one given in \cite[pp. 1059]{cormen2009introduction}. This would suggest that the methods are equally well suited to the task of computing a TS matrix, if it were further assumed that the TS matrices produced by each method were identical. This is not the case as seen in the following subsection.
%
\begin{figure}[tbp] 
    \centering 
    \scalebox{0.93}{\input{./figs/serial_speed.pgf}}
    \caption{Serial test series speed measurements.} 
    \label{fig:serial_speed}
\end{figure}
%
For the parallel simulations, averaged speed results appear in Figure~\ref{fig:multi_speed}. For this series, rather than varying the number of nonzero TS matrix elements, the number of compute cores used to solve the problem is increased, while the number of nonzero TS matrix elements remained constant at $\num{1.67E9}$ .  CS again was the slowest method, followed by CD, AD and then FD.  There is a somewhat surprise reversal of FD and AD for these parallel simulations.  Figure~\ref{fig:multi_speed} indicates that when looking at the average iteration time, the factor of speed improvement scales inversely with the number of compute cores used for the given conditions of this group of tests. Figure~\ref{fig:multi_speed} clearly shows that when core count doubles, compute time halves, for each method. Again, these results show that each of the methods as implemented in \emph{Peridigm} scale well when additional processors are used. 
%
\begin{figure}[tbp] 
    \centering 
    \scalebox{1.1}{\input{./figs/multi_speed.pgf}}
    \caption{Multicore test series speed measurements.} 
    \label{fig:multi_speed}
\end{figure}
%
A speculative explanation as to why FD and AD have reversed places in the speed tests with respect to the serial results is that the AD data-structures carry a list of partial derivatives of the functional evaluations along side the functional evaluations themselves only to be processed at the end of the tangent-stiffness matrix calculation when the derivatives are requested. This represents additional load on the hardware memory subsystem, such that may exceed that of FD yet still falls under a supposed memory bandwidth threshold during serial operation. Cores within a multicore processor use the same shared bus to access memory, such that cores compete for memory access. Supposing that exceeding the memory bandwidth threshold penalizes additional requested bandwidth, it is likely that the decrease in available memory bandwidth per core caused by parallel operation coupled with AD's additional resource requirements causes the AD method to be slower than FD. 

\subsection{Accuracy data}
%
Data for accuracy measurements were taken according to the methods mentioned in Section~\ref{tcs}, and reduced using equation~(\ref{eqn:accuracy}). The averaged results of these accuracy measurements for the serial tests can be seen in Figure~\ref{fig:serial_accuracy}. The averaged Frobenius norm (referred to as $l^2$-norm in the figure labels for conciseness) of the element-wise difference between a given TS matrix and the TS matrix produced by the AD method is plotted as a function of the number of nonzero TS matrix elements. CS was shown to be the most accurate of the methods when compared to AD and was roughly \emph{$6$ orders of magnitude more accurate than CD} and \emph{$9$ orders of magnitude more accurate than FD}. This accuracy ranking order was maintained for each test in the series. It should be pointed out that compared to the $l^2$-norms of any of the TS matrices by themselves, the magnitude of the accuracy metric calculated with equation~\ref{eqn:accuracy} for any of the methods is relatively small. Relative error was computed from these measurements and is reported in Table~\ref{tab:RelAcc}. The values for the $l^2$-norms of the TS matrices appear in the simulation data which is available in the paper repository. The effect of these accuracy differences on convergence rate of the Newton solver was studied and the results are reported in Section~\ref{sec:PeridigmConvergenceStudy}.  
%
The accuracy of FD and CD could be improved by using a smaller probe distance, $h$; however, if $h$ is too small, then the methods can suffer from severe effects, including solution divergence, of the round-off error associated with subtracting two numbers that are very close to one another.  The value of $h$ used in this study is the default value used in \emph{Peridigm} which is heuristically derived based on computational node spacing using the method explained in \cite[pp. 90]{ref-Adaggio}, as it is not a user determined setting in \emph{Peridigm}.  To summarize, a spacing of $1.E-6$ times the grid spacing is used to provide acceptable accuracy while preventing roundoff error. One of the beauties of the CS method is that since dependence on $h$ decreases for small $h$, which a numerical example in \cite[Table 1]{squire1998using} demonstrates, $h$, has no lower limit for effective values, therefore it can be set to machine epsilon and the analysis performed without the issues associated with round-off error. The value chosen for $h$ for CS used in this study was $\num{1.E-100}$ to demonstrate this.
%
\begin{figure}[tbp] 
    \centering
    \scalebox{1.0}{\input{./figs/serial_accuracy.pgf}} 
    \caption{Serial test series accuracy measurements.} 
    \label{fig:serial_accuracy} 
\end{figure}
%
The averaged results of the accuracy measurements taken for the parallel test series can be seen in Figure~\ref{fig:multi_accuracy}. The ordinate axis units remain unchanged from Figure~\ref{fig:serial_accuracy}, but the abscissa now indicates number of compute cores used to solve the test problem. Accuracy trends matched those for the single core tests. 
%
\begin{figure}[tbp] 
    \centering
    \scalebox{0.9}{\input{./figs/multi_accuracy.pgf}} 
    \caption{Multicore test series accuracy measurements.} 
    \label{fig:multi_accuracy} 
\end{figure}
%
\todo[]{Re: II.22} It can be seen in Figures~\ref{fig:serial_accuracy} and \ref{fig:multi_accuracy} that there appears to be some fluctuating behavior in the accuracy of the methods. Those figures are depicting absolute error. If relative error is computed from the absolute error measurements by dividing them by the Frobenius norm of the appropriate AD produced TS matrix, it can be seen as in Table~\ref{tab:RelAcc}, that there is little variation in the accuracy of the methods between tests or between serial and parallel test groups. 
%
\begin{table*}[tbp]    
  \scriptsize
  \centering
        \caption{Relative accuracy results} \label{tab:RelAcc}   
       \begin{tabular}{c c c c c}
         \toprule
         Cores & TS Elements & \multicolumn{3}{c}{Percent error} \\ 
         & $\times 10^6$ & CD & CS & FD \\
        \midrule
        1 & $\num{1.99}$ &  $\num{1.28E-8}$ &  $\num{2.29E-15}$ &  $\num{1.46E-5}$ \\
        1 & $\num{4.25}$ &  $\num{8.86E-8}$ &  $\num{2.25E-14}$ &  $\num{1.34E-5}$ \\
        1 & $\num{7.79}$ &  $\num{1.37E-8}$ &  $\num{2.27E-14}$ &  $\num{1.25E-5}$ \\
        1 & $\num{15.4}$ &  $\num{4.33E-8}$ &  $\num{2.52E-14}$ &  $\num{1.16E-5}$ \\
        1 & $\num{20.0}$ &  $\num{7.97E-8}$ &  $\num{2.18E-15}$ &  $\num{1.13E-5}$ \\
        1 & $\num{31.4}$ &  $\num{1.44E-8}$ &  $\num{2.29E-15}$ &  $\num{1.09E-5}$ \\
        1 & $\num{40.1}$ &  $\num{1.36E-7}$ &  $\num{2.24E-15}$ &  $\num{1.05E-5}$ \\
        1 & $\num{83.9}$ &  $\num{1.30E-7}$ &  $\num{2.44E-14}$ &  $\num{9.63E-6}$ \\
        1 & $\num{165.0}$ &  $\num{1.30E-7}$ &  $\num{2.44E-14}$ &  $\num{9.63E-6}$ \\
        32 & $\num{1670}$ &  $\num{6.00E-7}$ &  $\num{2.29E-14}$ &  $\num{6.93E-6}$ \\
        64 & $\num{1670}$ &  $\num{6.01E-7}$ &  $\num{2.31E-14}$ &  $\num{7.02E-6}$ \\
        96 & $\num{1670}$ &  $\num{6.03E-7}$ &  $\num{2.33E-14}$ &  $\num{7.10E-6}$ \\
        128 & $\num{1670}$ &  $\num{5.97E-7}$ &  $\num{2.34E-14}$ &  $\num{7.19E-6}$ \\
        \bottomrule
    \end{tabular}
\end{table*}

\subsection{Efficiency data}
%
Efficiency, calculated as the average number of seconds taken per nonzero tangent-stiffness matrix element, was plotted as a function of the number of nonzero TS matrix elements. The single core results are shown in Figure~\ref{fig:serial_efficiency}.  The ranking order of the methods matched their speed ranking in Figure~\ref{fig:serial_speed}. This plot indicates that efficiency is insensitive to the number of nonzero TS elements.  
%
\begin{figure}[tbp]
  \centering
  \scalebox{0.9}{\input{./figs/serial_speed_rel.pgf}}
  \caption{Serial test series efficiency measurements.}
  \label{fig:serial_efficiency}
\end{figure}
%
The parallel efficiency results are shown in Figure~\ref{fig:multi_efficiency}. The parallel efficiency plot shows that AD was slower than FD because for the given test problem it had a lower efficiency. Strangely AD appears to gain in efficiency over the last few data points in the plot. The reason behind this trend was not determined.
%
\begin{figure}[tbp]
  \centering
  \scalebox{0.9}{\input{./figs/multi_speed_rel.pgf}}
  \caption{Multicore test series efficiency measurements.}
  \label{fig:multi_efficiency}
\end{figure}



\section{\todo[]{Re: I.5}Convergence and displacement accuracy studies.} 
\label{sec:PeridigmConvergenceStudy}
%
While the previous sections profiled the per-iteration performance of each of the methods, it was necessary to conduct convergence rate studies in order to get a complete picture. It is necessary for an analyst to know if the accuracy differences measured in the per-iteration tests would amount to differences in convergence rates among the methods if they were allowed to solve problems normally. The question is posed, given that the step-size $h$ is heuristically selected by \emph{Peridigm} for the finite difference methods and presumably makes a good choice, do the single-iteration tangent-stiffness accuracy results predict the ranking of the methods in terms of convergence rate for a given study? For brevity, the convergence rate was measured for each of the methods in solving just the largest problem of the single-core test series. That problem was solved with each of the methods both on a single core and on all sixteen cores of a  node on \emph{Stampede}. As was seen in the per-iteration results, tangent-stiffness accuracy did not vary greatly from test to test, such that any pair of tests from the two series could have been considered reasonably representative. As in the validation problem shown in Section~\ref{sec:Validation}, would each of the methods return an equal convergence rate so long as $h$ were optimally selected? 

The results shown in Table~\ref{tab:ConvergenceStudy2} indicate that this was the case.  The difference in tangent-stiffness accuracy between the methods seen in the per-iteration tests was not sufficient to lead to differences in the convergence rate of the methods.
%
\begin{table}[htb]   
    \centering 
    \caption{Average iterations / load-step} 
    \label{tab:ConvergenceStudy2}   
    \begin{tabular}{c c c c c}
        \toprule 
        Test type & AD & CS & CD & FD\\
        \midrule 
        single-core & 63 & " & " & "\\ 
        multi-core  & 63 & " & " & "\\ 
        \bottomrule 
    \end{tabular} 
\end{table}

The end displacement results shown in Section~\ref{sec:Validation} indicated that up to a certain precision the solutions produced by each of the methods were identical. It was examined if this was the case for the solutions produced in \emph{Peridigm}. Here each of the solutions produced by each of the methods were compared to the AD produced solutions, under the assumption that the AD produced solutions would be closest to the solution produced by using an `exact' tangent-stiffness. Table~\ref{tab:PeridigmSolutionAccuracy} shows maximum force density discrepancy, indicating that the methods did indeed produce differing solutions in terms of nodal force density. The values for Table~\ref{tab:PeridigmSolutionAccuracy} were generated using a helper program included with \emph{Trilinos} called \emph{exodiff} which is used for automatically comparing \emph{exodus} output files. While differences in force density were detected, the magnitude of the greatest of them was $9$ orders of magnitude smaller than the relevant nodal force density values being differenced. 
%
\begin{table}[hbp]   
    \centering 
    \caption{Nodal force density maximum difference} 
    \label{tab:PeridigmSolutionAccuracy}   
    \begin{tabular}{c c c c}
        \toprule  & CS & CD & FD\\
        \midrule  Order of magnitude & $\num{1.E-8}$ & $\num{1.E-8}$ & $\num{1.E-5}$ \\ 
        \bottomrule 
    \end{tabular} 
\end{table}

The solution accuracy results do follow the same ranking seen in the tangent-stiffness accuracy results; however, again as in the validation problem the results differ only slightly. Speculatively this degree of error may be important to those running probabilistic simulations such that the propagation of error may be a concern. Also it is conceivable that if this error is a fixed quantity, insensitive to material properties or loading conditions, it may have a greater impact on simulations involving very low force densities such as for small displacements or very compliant materials. In this case it could be recommended that forward difference be avoided. 
%
\section{\todo[]{Re: I.5}Conclusions.}
\label{sec:Conc}
%
%Let's leave this section out and see what the reviewer says.  The paper is getting pretty long.
%
%\subsection{For further studies of tangent-stiffness accuracy in Newton's method}
%\label{subsec:FurtherStudies}
%The question is raised; what is the significance of tangent-stiffness estimation accuracy to the convergence of Newton's method?  The per-iteration and even convergence studies shown here indicate the effect of tangent-stiffness accuracy for only a single datum, a very accurate tangent-stiffness estimate, while the validation problem of Section~XX indicates the effect of using a grossly inaccurate tangent-stiffness. The results here show that for very small tangent-stiffness estimate inaccuracies, convergence rate is not affected, while for grossly inaccurate tangent-stiffnesss Newton's method fails.  Additional confounding factors include that a tangent-stiffness is only first order approximation of an often curved energy surface and that a non-naive implementation of Newton's method includes a line search phase that helps in choosing an optimal iterate given a tangent-stiffness.  Is it worthwhile then to investigate the effect of using moderately inaccurate tangent-stiffnesss, produced deliberately or otherwise, on the convergence rate of Newton's method?

%The answer may be no at least unless a more highly nonlinear class of problem is selected for study or a controllable method of producing a moderately inaccurate tangent-stiffness is found along with a reason for using it. While it is possible to incorrectly apply finite differencing and CS by choosing an inappropriate step-size, there is no reason that this should be done. The experiments show that step-size $h$ can be properly selected and that the methods are capable of producing accurate tangent-stiffnesss. It should be noted that the model used to perform the simulation could always be substituted for a more highly nonlinear one and perhaps this would somewhat increase the importance of tangent-stiffness accuracy. The study presented here failed to pose the question of how wrong a correctly implemented tangent-stiffness calculation scheme could go for plastic or otherwise nonlinear material models, here instead nonlinearity was imposed by the nonlocal interaction between material points essential to peridynamic models, like the linear model used in this study. 

%Nevertheless, Newton's method is not defined for using an inaccurate or 'wrong' tangent-stiffness, so using an inaccurate tangent-stiffness is failing to perform Newton's method by definition. But it is also true that it should not be expected that every differentiation technique performs flawlessly under all situations. The pragmatic strategy to study this may be to compare a prospective method's tangent-stiffness accuracy to that produced by AD and develop a rule of thumb based on observed convergence rates from prior simulations as they relate to the percent difference of the prospective and AD produced tangent-stiffnesss. Also it is possible to calculate a sensitivity of tangent-stiffness error to step-size alongside a sensitivity of convergence rate to step-size, and then to relate these two quantities as a map of tangent-stiffness error to convergence rate change from ideal. Additionally it is still possible to look at the convergence conditions for Newton's method and suggest a relationship between tangent-stiffness accuracy and convergence rate in a theoretical manner. To this point, there are a set of convergence conditions derived in \cite[Chap. 2, p.41]{burkeLectures} which relate quality of tangent-stiffness inverse approximations to convergence rate classification, here reproduced,
%%
%\begin{equation} \mid \mid (r \prime (x_{k})^{-1} - J_{k}^{-1})y_{k} \mid \mid \leq K,
%\end{equation}

%for worse than linear convergence,

%\begin{equation} \mid \mid (r \prime (x_{k})^{-1} - J_{k}^{-1})y_{k} \mid \mid \leq \Theta_{1}^{k}K; \Theta_{1} \in (0,1),
%\end{equation}

%for linear convergence,

%\begin{equation} \mid \mid (r \prime (x_{k})^{-1} - J_{k}^{-1})y_{k} \mid \mid \leq min{M_{2} \mid \mid x_{k} - x_{k-1} \mid \mid, K}; M_{2} > 0
%\end{equation}

%for two step quadratic,

%\begin{equation} \mid \mid (r \prime (x_{k})^{-1} - J_{k}^{-1})y_{k} \mid \mid \leq min{M_{3} \mid \mid r(x_{k}) \mid \mid, K}; M_{3} > 0
%\end{equation}

%and lastly for quadratic convergence. Here $r \prime$ is the true system
%tangent-stiffness and $J$ is the approximation. Essentially the conditions measure how
%well the difference of the true tangent-stiffness inverse and approximation works as
%a contraction map within the closure of a radius of convergence about the true
%solution. This property is necessary for the solution method using the
%approximation to converge rather than to diverge, as is true of the exact Newton
%method.

%It may be interesting to artificially perturb the tangent-stiffness calculations with
%random error in order to better control experimentally the onset of inaccuracy
%to develop a sensitivity of convergence rate to this type of error. Perhaps an
%analyst could use this knowledge to devise an adaptive differentiation
%technique selection method which economizes on accuracy until a significant
%tangent-stiffness error is measured during a special doubly calculated iteration.

%Additionally, there are Quasi-Newton methods such as Broyden's method which
%uses an approximate inverse tangent-stiffness and updates it with information from the 
%iterate and residual \cite{dennis1971convergence}. A method called
%Incomplete tangent-stiffness Newton's method may the closest to the idea of deliberately
%making use of an inaccurate tangent-stiffness in the sense that arbitrary elements of
%the full tangent-stiffness are set to zero to reduce computational effort
%\cite{liu2008incomplete}. The Quasi-Newton methods represent a compromise of
%Newton's method for the promise of faster iterations in exchange for a lower
%convergence rate. For both of these methods, the iterate and residual are
%computed as in Newton's method. As to the question of algorithmic consistency,
%these numerical methods evaluate the same linearized constitutive law and
%governing equation as for the exact Newton's method which in the case of a
%plastic simulation are involved in iteratively determining algorithmic tangent moduli. 
%In fact, a secant equation based method is appropriate for Newton's method with algorithmic
%tangent modulii since the tangent-stiffness ought only depend on known converged
%values such that the Newton iterate could not artificially drive the plasticity
%of the model \cite[Chap. 6]{belytschko1999nonlinear}. It should be noted that
%constraints or a line search algorithm to minimize the residual are necessary
%whereas the pure secant method does not return unique solutions in
%multi-dimensions.
%

\subsection{Remarks on speed results}
%
The per-iteration speed results shown here indicate that the AD method is faster than CD or FD.  While this may be the case for the implementation in the released version of \emph{Peridigm} this is not reflective of any limitation of these methods. In a side experiment, the finite difference code in \emph{Peridigm} was re-written to take advantage of the benefits of storing the results of duplicated intermediate calculations. These modifications were general, low memory footprint, and could be adopted in any numerical simulation software as good practice. An informal performance test run was performed on the largest single core test from the main study, instead run on four compute cores. Table~\ref{tab:PerformanceMod} shows the timing results for AD as reference along with an unmodified CD, and then modified CD and FD. Unmodified FD and the CS methods do not appear in the comparison since the modification was not implemented for the CS method and timing results for the unmodified methods appear, as for a different computer system, in Table~\ref{tab:results}. 
%
\begin{table}[tbp]   
\centering 
\caption{Total time spent computing tangent-stiffness} 
\label{tab:PerformanceMod}   
\begin{tabular}{c c c c c}
\toprule & AD & CD & Modified CD & Modified FD\\
\midrule  Total seconds & $~430$ & $~570$ & $~430$ &$~320$ \\ 
\bottomrule 
\end{tabular} 
\end{table}

The results indicated that in exchange for a modest programming effort, a performance increase of around a quarter was available for the finite difference methods and that FD could eclipse AD. What allowed the performance modifications to enhance the finite difference methods was their lack of sophistication compared to AD in that their explicitly laid out and repetitive code could be readily optimized. 

\subsection{Final remarks}
%
Based on the computational experiments, it appears that for users of \emph{Peridigm} there is a case to be made for using AD for reasons of speed. However, certain parallel tests done in this study showed a small performance gain over AD with the FD method.  The results showed that CS produced more accurate tangent-stiffness matrices than CD and FD under the parameters of the tests; however, it was determined that this difference in accuracy was not sufficient to alter the convergence rate for a test problem.  
%In order for tangent-stiffness accuracy to have an impact on convergence rate, and therefore serve as a predictor of convergence rate in per-iteration results, the amount of error detected between a supposed perfectly accurate tangent-stiffness and the test tangent-stiffness needs to be greater than allowed by the conditions seen in \cite{burkeLectures}, reproduced in subsection~\ref{subsec:FurtherStudies}. 

The CS method was shown to be highly accurate in calculating tangent-stiffness matrices by the standards of this study, and it is slower than other methods for a naive implementation. In the context of future use of CS in \emph{Peridigm}, CS holds the advantage of relying on byte-copyable\footnote{In this context,  byte-copyable means that the collection of data to transfer is made of identically sized pieces.} data types, while AD requires a complicated serialization and deserialization in order to function as byte-copyable. This is fine for copying in between MPI processes, but deserialization on an accelerator or GPU is very difficult, as this would require compiling a version of the AD library being used that is compatible with the accelerator or GPU being used.  Byte-copyability is significant for example because it is a requirement for the basic offload mode provided for Intel MIC accelerator boards which are installed in \emph{Stampede}, where the alternative mode requires more extensive effort \cite{intel_byte_copyable}. What this means is that if \emph{Peridigm} or any simulation software were to be reformulated to take advantage of accelerators or GPUs, the flexibility of complex-step owed to its simple implementation may make it more viable for that application given its high accuracy. Additional performance gains may be achieved through the use of the templated \emph{TPetra} package in \emph{Trilinos} which would allow the use of complex data types natively, and alleviate the expense of the dynamic-casts that were utilized in this study.  As a final comment, it should be noted that from a code development perspective the CS method may be the easiest of the methods to implement; it only requires one functional evaluation, there is no heuristic choice of probe distance, and there is no dependence on an external AD library.  From this perspective, the method may still be an attractive method for tangent-stiffness matrix evaluation. However, at least for the models producing the results shown here, each of the methods returned quadratic convergence, indicating that each was sufficiently accurate. ``Marching orders'' for the analyst would be to select the swiftest method, because it appears that a greater accuracy than what is adequate does not enhance the convergence rate or iteration speed until either classical Newton's convergence conditions or the approximate case conditions are violated. 

\section{Acknowledgements.}
\label{sec:ack}
This work was funded in part by grants from the United States Air Force Office of Scientific Research grant number W911NF-11-1-0208 and National Energy Technology Laboratory grant number DE-FE0010808. The authors acknowledge the Texas Advanced Computing Center (TACC) at The University of Texas at Austin for providing HPC resources that have contributed to the research results reported within this paper. URL: \url{http://www.tacc.utexas.edu}.
%% References
%%
%% Following citation commands can be used in the body text:
%% Usage of \cite is as follows:
%%   \cite{key}         ==>>  [#]
%%   \cite[chap. 2]{key} ==>> [#, chap. 2]
%%

%% References with bibTeX database:

\section{References.}

\bibliographystyle{elsarticle-num}
\bibliography{all}

\appendix
\renewcommand*{\thesection}{\Alph{section}}
%
\section{Complex-Step example.}
\label{sec:appendixA}
%
Using $f(x) = \cos(x)$, we have
%
\[
f (x + i h) = \cos(x + i h),
\]
%
or through trigonometric identities equivalently
%
\[
f(x + i h) = \cos(x) \cosh(h) - i \sin(x) \sinh(h).
\]
%
Applying equation (\ref{eqn:complexFirstDeriv})
\[
    \frac{\partial f(x)}{\partial x} = - \frac{\sin(x) \sinh(h)}{h} + \mathcal{O}(h^2).
\]
Since $h$ is arbitrary we desire it to be as small as possible or
\[
\frac{\partial f(x)}{\partial x} = \lim_{h \to 0} - \frac{\sin(x) \sinh(h)}{h},
\]
using l'H\^opital's rule
\[
\frac{\partial f(x)}{\partial x} = \lim_{h \to 0} - \frac{\sin(x) \cosh(h)}{1}.
\]
As $h \to 0$ then $\cosh(h) \to 1$ thereby recovering the exact derivative
\[
\frac{\partial f(x)}{\partial x} =-\sin(x).
\]

\section{Automatic differentiation example.}
\label{sec:appendixB}

\begin{enumerate}
%Set up example problem. The example is meant to walk the reader through the forward AD algorithm
%while encouraging them to rely on their understanding of the chain rule
\item Take an example composition function $f(x) = (\sin(\cos(x)))^2$\\

\item Suppose we want to know $\frac{df}{dx} \mid_{x_0}$ where $x_0$ is a particular value of $x$. \\

\begin{enumerate}
%This first block of subitems is meant to establish that the values of x that the user cares about
%about evaluating functions for are included within the set of numbers that functions g,h,k are allowed 
%to deal with, and that nested functions can take each other as input, where s may not necessarily equal
%x.
\item Given that: \\
\label{given}
\begin{enumerate}
\item $S \in R^1$ 
\item $X \in S$ 
\item $g, h, k \in H: S \rightarrow S$ 
\item $\forall s \in S : g(s) = {s}^2$, $h(s) = \sin(s)$, $k(s) = \cos(s)$ 
\item
\label{composition}
$\forall x \in X : f(x) = g(h(k(x)))$ \\
\end{enumerate}

\item Given that the computer is programmed with some mathematical definitions:\\
\begin{enumerate}
\item
\begin{enumerate}
\item
%The computer has at least definitions for these functions and variables
$u, v, w \in H: R^3 \rightarrow R^1$ \\
\item
$s,a,b,x \in R^1$\\
\end{enumerate}

\item
%AD doesn't return symbolic expressions for derivatives that can be re-evaluated.
%Instead, generic versions of functions and evaluations of their partials are defined together.
particular values can be identified: \\
$s = s_0,..,s_n,...s_{\infty} \mid n=[0,\infty)$ \\ 
and similarly for the other variables $a,b,x$ \\
\item
functions $u,v,w$ and their partial derivatives w.r.t $s$ are defined such that: \\
\begin{tabular}{l}
%\multicolumn{2}{c}{for $a, b, s$ equal to $a_n, b_n, s_n$:} \\ \hline
for $a, b, s$ equal to $a_n, b_n, s_n$: \\ \hline
$u \mid _{a_n, b_n, s_n} = a_n \cdot s_n^{b_n}$ \\
$\frac{\partial{u}}{\partial{s}} \mid _{a_n, b_n, s_n} = a_n \cdot b_n \cdot s_n^{b_n - 1}$ \\
\\
$v \mid _{a_n, b_n, s_n} = a_n \cdot sin(b_n \cdot s_n)$ \\
$\frac{\partial{v}}{\partial{s}} \mid _{a_n, b_n, s_n} = a_n \cdot b_n \cdot cos(b_n \cdot s_n)$ \\ 
\\
$w \mid _{a_n, b_n, s_n} = a_n \cdot cos(b_n \cdot s_n)$ \\
$\frac{\partial{w}}{\partial{s}} \mid _{a_n, b_n, s_n} = -a_n \cdot b_n \cdot sin(b_n \cdot s_n)$\\
\end{tabular} 
\end{enumerate}
%a comment
\item 
\label{abcvalues}
%General definitions of functions are specialized by arguments. 
Given that it is possible to describe $f(x) = (\sin(\cos(x)))^2$ in terms the computer understands by inputting
$f(x)$ such that the computer stores an equivalent statement $f(x) = u(a, b, s) \mid_{arguments}$,
iff the arguments of $u,v,w$ are chosen such that $u,v,w$ approximate $g,h,k$ as follows:\\
\begin{tabular}{l l l l | c}
	Function & a & b & s & Approximates Function\\ \hline 
	$u$ & $1$ & $2$ & $v$ & $g$\\ \hline 
	$v$ & $1$ & $1$ & $w$ & $h$\\ \hline 
	$w$ & $1$ & $1$ & $x$ & $k$\\ \hline 	
\end{tabular}
\end{enumerate} 

\item 
\label{differentiation}
%AD is equivalent to the chain rule, when the computer has definitions for functions and partials
%that can be adapted to match the composition function.
It follows from \ref{composition} that we can evaluate $\frac{d}{d x}f(x) \mid_{x_0}$ with the chain rule: \\ \\
$\frac{d}{d x}f(x) = \frac{d}{d x} g(h(k(x))) \mid_{x_0}$ \\
$\frac{d}{d x}f(x) =  \frac{d{g}}{d{h}} \cdot \frac{d{h}}{d{k}}
\cdot \frac{d{k}}{d{x}}\mid_{x_0}$ \\ \\
From the rest of \ref{given} it follows that we can approximate  $\frac{d}{d x}f(x) \mid_{x_0}$ by
specializing the computer's general forms of $u, v, w$ 
according to~\ref{abcvalues}, with parameters $a, b$ chosen for each function as in~\ref{abcvalues} and held constant, and
evaluating, such that the total derivative of our original function w.r.t $x$ where $x = x_0$ is approximated
by the partial derivative of our equivalent statement, $f(x) = u(a, b, s) \mid_{arguments}$,  w.r.t $s$ 
where $s = x_0$. \\

\end{enumerate}
\label{doingthework}
%This shows how information flows through the nested functions in one expression, like a table of contents
For completeness we write out the computer's steps to evaluate \ref{differentiation} under the conditions of
\ref{abcvalues} with a particular value of $s = x_0$, in equation format: \\ \\
$\frac{\partial}{\partial x}f(x) \mid _{x_0} = \frac{\partial{u}}{\partial{v}} \mid _{1, 2, v \mid _{1, 1, w \mid _{ 1, 1, x_0}}} \cdot \frac{\partial{v}}{\partial{w}} \mid _{1,1, w \mid _{1, 1, x_0}} \cdot \frac{\partial{w}}{\partial{s}} \mid _{1, 1, x_0} \cdot \frac{ds}{dx}.$\\ \\
%Repeating above in tabular format: \\ \\
%This shows the same information but is ordered to show progression, like chapters
%\begin{tabular}{l l l}
%Current Evaluation & $s$ Value & Partial Derivative \\ \hline
%$w(a, b, s)\mid_{1, 1, s}$ & $x_0$ & $\frac{\partial{w}}{\partial{s}} \mid _{1, 1, x_0}$ \\
%$v(a, b, s)\mid_{1, 1, s}$ & $w(a, b, s)\mid_{1, 1, x_0}$ & $\frac{\partial{v}}{\partial{w}} \mid _{1,1, w \mid _{1, 1, x_0}} \cdot \frac{\partial{w}}{\partial{s}} \mid _{1, 1, x_0}$ \\
%$u(a, b, s)\mid_{1, 2, s}$ & $v(a, b, s)\mid_{1, 1, w(a, b, s)\mid_{1, 1, x_0}}$ & $\frac{\partial{u}}{\partial{v}} \mid _{1, 2, v \mid _{1, 1, w \mid _{ 1, 1, x_0}}} \cdot \frac{\partial{v}}{\partial{w}} \mid _{1,1, w \mid _{1, 1, x_0}} \cdot \frac{\partial{w}}{\partial{s}} \mid _{1, 1, x_0} \cdot 1$ \\
%\end{tabular} \\ \\
%This explains what the equation and table above mean.
Because the computer knew the analytical forms of the partial derivatives of each of $u,v,w$ beforehand, which of course needed to be specified using an AD data structure from a library like Trilinos::Sacado \cite{trilinos} or ADIFOR \cite{bischof1995adifor},
all it needed to do was:
\begin{enumerate}
\item to evaluate each of $u,v,w$ according to \ref{abcvalues}, in order from $w \rightarrow
v \rightarrow u$, 
\item to remember the values for $x_0$ and the output of each function evaluation besides  $u$, 
\item then to use $x_0$ and the output of the function evaluations as input for the corresponding partial
derivative function  evaluations, and
\item store the individual partials.
\end{enumerate}
Lastly, to compute the partial derivative of the entire composition function, the computer multiplies
the individual partials together in observance of the chain-rule. It is important to note that this example
was simplistic in that only the case of a single independent variable was covered, making the distinction of type of AD
less important. 

\end{document}
